{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ddbfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"../../train.csv\")\n",
    "X = df[\"Text\"].values\n",
    "y = df[\"Category\"].values\n",
    "m = 0.9 #proportion of data for training vs validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=m, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78d1cec",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae98f0",
   "metadata": {},
   "source": [
    "(d) NN. Consider a neural network with the following hyperparameters: the initial weights\n",
    "uniformly drawn in range [0,0.1] with learning rate 0.01.\n",
    "\n",
    "    ● Train a single hidden layer neural network using the hyperparameters on the training dataset, except for the number of hidden units (x) which should vary among 5, 20, and 40. Run the optimization for 100 epochs each time. Namely, the input layer consists of n features x = [x1, ..., xn]^T , the hidden layer has x nodes z = [z1, ..., zx]^T , and the output layer is a probability distribution y = [y1, y2]^T over two classes.\n",
    "\n",
    "    ● Plot the average training cross-entropy loss as shown below on the y-axis versus the number of hidden units on the x-axis. Explain the effect of numbers of hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "257ba117",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 96\u001b[0m\n\u001b[0;32m     91\u001b[0m     probs \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict_proba(vector_test)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m probs, clf\n\u001b[1;32m---> 96\u001b[0m main()\n",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m---> 10\u001b[0m     data \u001b[38;5;241m=\u001b[39m make_df(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m     y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategory\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     12\u001b[0m     test_data \u001b[38;5;241m=\u001b[39m make_df(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 63\u001b[0m, in \u001b[0;36mmake_df\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_df\u001b[39m(filename):\n\u001b[1;32m---> 63\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(filename)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\sharp\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\sharp\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\sharp\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\sharp\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\sharp\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "#SORRY THIS IS A MESS WE WILL CLEAN IT UP LATER DO NOT USE THIS CLASSIFIER IT IS BROKEN \n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def main():\n",
    "    data = make_df(\"train.csv\")\n",
    "    y = data[\"Category\"]\n",
    "    test_data = make_df(\"test.csv\")\n",
    "    y_test = test_data[\"Category\"]       \n",
    "    vector, vector_test = vectorize(data, test_data)\n",
    " #   make_classifier(vector, data, vector_test)\n",
    "\n",
    "    cels = []\n",
    "    num_nods = [5, 20, 40]\n",
    "    for num in num_nods:\n",
    "        probs, clf = make_classifier(vector, y, vector_test, num)\n",
    "        cel = calc_cels(probs)\n",
    "        cels.append(cel)\n",
    "\n",
    "    plt.plot(num_nods, cels, marker = \"*\", color = \"#F543BA\", markersize = 10)\n",
    "    plt.title(\"Cross entropy loss\")\n",
    "    plt.show()\n",
    "\n",
    "    ms = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    test_f1s = []\n",
    "    train_f1s = []\n",
    "\n",
    "    for m in ms:\n",
    "        train_f1, test_f1 = train_on_m(y, vector, vector_test, m, y_test)\n",
    "        test_f1s.append(test_f1)\n",
    "        train_f1s.append(train_f1)\n",
    "\n",
    "    \n",
    "    plt.plot(ms, test_f1s, marker = \"o\", color = \"#BA52FF\", markersize = 7)\n",
    "    plt.title(\"Test f1s\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(ms, train_f1s, marker = \"x\", color = \"#F431CB\", markersize = 8)\n",
    "    plt.title(\"Train f1s\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def vectorize(data, test_data):\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(data[\"Text\"])\n",
    "    print(f'vector vocabulary - {vectorizer.vocabulary}\\n')\n",
    "\n",
    "    vector = vectorizer.transform(data[\"Text\"])\n",
    "    test_vector = vectorizer.transform(test_data[\"Text\"])\n",
    "    \n",
    "    print(f'features\\n {vectorizer.get_feature_names_out()}\\n')\n",
    "\n",
    "    print(f'vector shape: {vector.shape}\\n')\n",
    "    print(f'article vector\\n {vector.toarray()}')\n",
    "\n",
    "    return vector, test_vector\n",
    "\n",
    "def make_df(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def calc_cels(probs):\n",
    "    total = 0\n",
    "\n",
    "    for pair in probs:\n",
    "        loss1 = pair[0] * math.log(pair[0])\n",
    "        loss2 = pair[1] * math.log(pair[1])\n",
    "        cel = - (loss1 + loss2)\n",
    "        total += cel\n",
    "\n",
    "    avg_cel = total / len(probs)\n",
    "\n",
    "    return avg_cel\n",
    "\n",
    "\n",
    "# passes in training xs, training ys, testing xs\n",
    "''' THIS IS THE CLASSIFIER FUNCTION YOU SHOULD MIRROR FOR YOUR MODEL FOR IT TO WORK WITH TASK 3\n",
    "    IGNORE num_nodes that is specific to the NN classifier. \n",
    "'''\n",
    "def make_classifier(vector, y, vector_test, num_nodes = 5): \n",
    "\n",
    "    clf = MLPClassifier(activation = 'relu', solver = 'sgd', learning_rate_init = 0.01,\n",
    "                        max_iter = 100, hidden_layer_sizes = (num_nodes,)).fit(vector, y)\n",
    "\n",
    "\n",
    "    probs = clf.predict_proba(vector_test)\n",
    "\n",
    "    return probs, clf\n",
    "        \n",
    "\n",
    "main()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# hyperparams; #hidden layers, #neurons per layer, #activation function\n",
    "\n",
    "# adj initial weight, adj learning rate, adj # epoch,\n",
    "#adj # hidden units (1layer has x units)\n",
    "\n",
    "\n",
    "\n",
    "#act function relu\n",
    "#solver sgd\n",
    "#alpha leave default\n",
    "#learning_rate_init: set to 0.01\n",
    "#max_iter: 100 (#epochs)\n",
    "#\n",
    "\n",
    "# got a warning.warn from warnings which told us there was a warning about convergance, stopped training bc max iter = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6639512b",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72002d5",
   "metadata": {},
   "source": [
    "(a) We explore how the size of the training data set affects the test and train accuracy. For each\n",
    "value of m in [0.1, 0.3, 0.5, 0.7, 0.9], train your classifier on the first m portion of the training\n",
    "examples (that is, use the data given by XTrain[0:mN] and yTrain[0:mN]). Please report two\n",
    "plots: (i) training and (ii) testing accuracy for each such value of m with the x-axis referring to m\n",
    "and the y-axis referring to the classification accuracy in 𝐹1 measure as shown below. In total,\n",
    "there should be four curves for training accuracy and four curves for testing accuracy. Explain\n",
    "the general trend of the two plots in terms of training and testing accuracy if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "753e48e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_m(train_y, vector, vector_test, m, y_test):\n",
    "    \n",
    "    sub_vector = vector[0:round(vector.shape[0]*m)]\n",
    "    \n",
    "    probs, clf = make_classifier(sub_vector, train_y[0:round(len(train_y)*m)], vector_test)\n",
    "\n",
    "    preds_test = clf.predict(vector_test)\n",
    "\n",
    "    preds_train = clf.predict(sub_vector)\n",
    "\n",
    "    train_f1 = calc_f1(preds_train, train_y)\n",
    "    test_f1 = calc_f1(preds_test, y_test)\n",
    "\n",
    "    return train_f1, test_f1\n",
    "\n",
    "def calc_f1(preds, actual):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "\n",
    "# tech is positive\n",
    "\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] == \"tech\" and actual[i] == \"tech\":\n",
    "            tp += 1\n",
    "        elif preds[i] == \"tech\" and actual[i] == \"entertainment\":\n",
    "            fp += 1\n",
    "        elif preds[i] == \"entertainment\" and actual[i] == \"entertainment\":\n",
    "            tn += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2*((precision*recall) / (precision+recall))\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d8471a",
   "metadata": {},
   "source": [
    "(b) Let’s use 5-fold cross-validation to assess model performance. Investigate the impact of key\n",
    "hyperparameters of your choices for each classifier using a testing dataset. E.g., for SVM, the\n",
    "classification accuracy may be significantly affected by the kernels and hyperparameter\n",
    "combination. List hyperparameters for each classifier and demonstrate how these\n",
    "hyperparameters impact on the testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c523aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector vocabulary - None\n",
      "\n",
      "features\n",
      " ['00' '000' '000th' ... 'zooms' 'zooropa' 'zorro']\n",
      "\n",
      "vector shape: (428, 13518)\n",
      "\n",
      "article vector\n",
      " [[0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Scores for normal classifier: [1.         0.9767316  0.9418526  1.         0.97646733] with average 0.9790103071920611\n",
      "LEARNING RATE\n",
      "Score for learning rate 0.001: [1.         1.         0.89903846 0.84444444 0.95194508] with average 0.9390855972148879\n",
      "Score for learning rate 0.005: [1.         0.95058824 1.         0.89903846 0.85714286] with average 0.9413539107950873\n",
      "Score for learning rate 0.01: [1.         0.95058824 1.         0.84444444 1.        ] with average 0.9590065359477125\n",
      "Score for learning rate 0.05: [1.         0.95058824 1.         0.89903846 1.        ] with average 0.9699253393665159\n",
      "Score for learning rate 0.1: [0.95206972 0.95058824 0.89903846 0.89903846 1.        ] with average 0.9401469750293281\n",
      "Score for learning rate 0.5: [1.         0.95058824 0.85714286 0.95058824 1.        ] with average 0.9516638655462184\n",
      "Score for learning rate 0.75: [1.         0.95194508 0.95194508 0.78571429 1.        ] with average 0.9379208891794704\n",
      "Score for learning rate 1: [1.         0.90277778 1.         0.95058824 0.80909091] with average 0.9324913844325609\n",
      "Score for learning rate 2: [1.         0.90454545 0.89903846 0.90277778 0.65      ] with average 0.8712723387723388\n",
      "Score for learning rate 5: [0.37142857 0.36363636 0.36363636 0.36363636 0.36363636] with average 0.3651948051948052\n",
      "MAX ITERATIONS\n",
      "Score for max iterations 5: [0.95206972 0.80909091 0.90454545 0.56891496 0.85714286] with average 0.81835277871331\n",
      "Score for max iterations 50: [1.         0.95058824 1.         0.78571429 1.        ] with average 0.9472605042016806\n",
      "Score for max iterations 100: [0.95206972 0.95058824 0.95058824 0.89903846 1.        ] with average 0.9504569297804594\n",
      "Score for max iterations 200: [0.95206972 0.95058824 1.         0.84444444 1.        ] with average 0.9494204793028324\n",
      "Score for max iterations 500: [0.90178571 0.95058824 1.         0.89903846 1.        ] with average 0.9502824822236586\n",
      "Score for max iterations 1000: [1.         0.95058824 0.78571429 0.84444444 1.        ] with average 0.9161493930905695\n",
      "Score for max iterations 5000: [1.         0.95058824 0.95058824 0.84444444 1.        ] with average 0.9491241830065359\n",
      "ACTIVATION FUNCTION\n",
      "Score for activation function relu: [0.95206972 0.84444444 1.         0.89903846 1.        ] with average 0.9391105245517011\n",
      "Score for activation function identity: [0.95206972 0.95058824 1.         0.84444444 1.        ] with average 0.9494204793028324\n",
      "Score for activation function logistic: [1.         0.95058824 0.95058824 0.84444444 1.        ] with average 0.9491241830065359\n",
      "Score for activation function tanh: [1.         0.95058824 1.         0.84444444 1.        ] with average 0.9590065359477125\n",
      "HIDDEN LAYERS\n",
      "Score for number of layers =  1: [1.         0.95058824 0.95058824 0.89903846 1.        ] with average 0.9600429864253395\n",
      "Score for number of layers =  2: [0.95206972 0.95058824 0.95194508 0.84444444 1.        ] with average 0.9398094953211389\n",
      "Score for number of layers =  3: [1.         0.95058824 0.78571429 0.95194508 1.        ] with average 0.9376495202199873\n",
      "Score for number of layers =  4: [0.90178571 0.72148541 1.         0.84444444 0.36363636] with average 0.7662703867014212\n",
      "Score for number of layers =  5: [1.         1.         0.36363636 0.95194508 0.80909091] with average 0.8249344705637611\n",
      "Score for number of layers =  10: [0.37142857 0.36363636 0.36363636 0.36363636 0.36363636] with average 0.3651948051948052\n",
      "Score for number of layers =  20: [0.37142857 0.36363636 0.36363636 0.36363636 0.36363636] with average 0.3651948051948052\n",
      "Score for number of layers =  40: [0.37142857 0.36363636 0.36363636 0.36363636 0.36363636] with average 0.3651948051948052\n"
     ]
    }
   ],
   "source": [
    "#use 5-fold CV to assess performance, experiment w diff hyperparams\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def vectorize(data, test_data):\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(data[\"Text\"])\n",
    "    print(f'vector vocabulary - {vectorizer.vocabulary}\\n')\n",
    "\n",
    "    vector = vectorizer.transform(data[\"Text\"])\n",
    "    test_vector = vectorizer.transform(test_data[\"Text\"])\n",
    "    \n",
    "    print(f'features\\n {vectorizer.get_feature_names_out()}\\n')\n",
    "\n",
    "    print(f'vector shape: {vector.shape}\\n')\n",
    "    print(f'article vector\\n {vector.toarray()}')\n",
    "\n",
    "    return vector, test_vector\n",
    "\n",
    "def make_df(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    data = make_df(\"../../train.csv\")\n",
    "    y = data[\"Category\"]\n",
    "    test_data = make_df(\"../../test.csv\")\n",
    "    y_test = test_data[\"Category\"]       \n",
    "    vector, vector_test = vectorize(data, test_data)\n",
    "    probs, model = make_classifier(vector, y, vector_test)\n",
    "\n",
    "    cross_validate(model, vector, y, vector_test, y_test)\n",
    "\n",
    "    # passes in training xs, training ys, testing xs\n",
    "def make_classifier(vector, y, vector_test, num_nodes = 5): # discuss num nodes in report\n",
    "\n",
    "    clf = MLPClassifier(activation = 'relu', solver = 'sgd', learning_rate_init = 0.01,\n",
    "                        max_iter = 100, hidden_layer_sizes = (num_nodes,)).fit(vector, y)\n",
    "\n",
    "\n",
    "    probs = clf.predict_proba(vector_test)\n",
    "\n",
    "    return probs, clf\n",
    "\n",
    "def cross_validate(clf, train_descriptions, train_y, test_descriptions, test_y):\n",
    "    # use kfold validation (k = 5).\n",
    "    # hyperparameters that we used:\n",
    "    \n",
    "    scores = cross_val_score(clf, train_descriptions, train_y, cv=5, scoring=\"f1_macro\")\n",
    "\n",
    "    print(\"Scores for normal classifier:\", scores, \"with average\", str(sum(scores) / len(scores)))\n",
    "\n",
    "    # LEARNING RATE\n",
    "    # very low learning rate had bad accuracy suggesting that it does not converge to minima with that epoch count\n",
    "    # marginal change, but incr learning rate caused slightly lower avg perf, so better to keep small (not too small for computations sake.\n",
    "    print(\"LEARNING RATE\")\n",
    "    learning_rates = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 0.75, 1, 2, 5]\n",
    "    for learning_rate in learning_rates:\n",
    "        # retrain clf\n",
    "        clf = MLPClassifier(activation = 'relu', solver = 'sgd', learning_rate_init = learning_rate,\n",
    "                        max_iter = 100, hidden_layer_sizes = (5,)).fit(train_descriptions, train_y)\n",
    "        get_scores(clf, test_descriptions, test_y, \"learning rate\", learning_rate)\n",
    "\n",
    "    # MAX ITER\n",
    "    # for very low values of max_iter_val, we get a convergence warning\n",
    "    # always for 5 and 50, sometimes for 100, never for bigger than that\n",
    "    # really large epoch values don't give much noticeable change to the accuracy\n",
    "    # therefore suggesting there has been convergence before that many epochs and we are running unnecessarily\n",
    "    # so we will use the default from scikit learn of 200 epochs.\n",
    "    print(\"MAX ITERATIONS\")\n",
    "    max_iter_values = [5, 50, 100, 200, 500, 1000, 5000]\n",
    "    for max_iter_val in max_iter_values:\n",
    "        # retrain clf\n",
    "        clf = MLPClassifier(activation = 'relu', solver = 'sgd', learning_rate_init = 0.01,\n",
    "                        max_iter = max_iter_val, hidden_layer_sizes = (5,)).fit(train_descriptions, train_y)\n",
    "        get_scores(clf, test_descriptions, test_y, \"max iterations\", max_iter_val)\n",
    "\n",
    "    # ACTIVATION FUNCTION\n",
    "    # doesn't seem like there's much change beyond natural variation\n",
    "    # all between 97 and 99 which we have seen similar scores from relu alone\n",
    "    # might be worth looking at sigmoid vs relu for this context to justify continuing with relu\n",
    "    print(\"ACTIVATION FUNCTION\")\n",
    "    activation_functions = [\"relu\", \"identity\", \"logistic\", \"tanh\"]\n",
    "    for function in activation_functions:\n",
    "        # retrain clf\n",
    "        clf = MLPClassifier(activation = function, solver = 'sgd', learning_rate_init = 0.01,\n",
    "                        max_iter = 100, hidden_layer_sizes = (5,)).fit(train_descriptions, train_y)\n",
    "        get_scores(clf, test_descriptions, test_y, \"activation function\", function)\n",
    "\n",
    "    # HIDDEN LAYERS\n",
    "    # increasing number of hidden layers for this decreased accuracy, and also gave us warning from warnings.warn convergence warning\n",
    "    # this meant that the optimal number of hidden layers for 100 epoch is 1\n",
    "    # but may need fine tuning alongside other epoch numbers\n",
    "    print(\"HIDDEN LAYERS\")\n",
    "    num_layers = [1, 2, 3, 4, 5, 10, 20, 40]\n",
    "    for num in num_layers:\n",
    "        # retrain clf\n",
    "        sizes = (5,) * num\n",
    "        clf = MLPClassifier(activation = 'relu', solver = 'sgd', learning_rate_init = 0.01,\n",
    "                        max_iter = 100, hidden_layer_sizes = sizes).fit(train_descriptions, train_y)\n",
    "        get_scores(clf, test_descriptions, test_y, \"number of layers = \", num)\n",
    "\n",
    "\n",
    "def get_scores(clf, test_descriptions, test_y, hyperparam, value):\n",
    "        scores = cross_val_score(clf, test_descriptions, test_y, cv=5, scoring=\"f1_macro\")\n",
    "        avg = sum(scores) / len(scores)\n",
    "        print(f\"Score for {hyperparam} {value}:\", scores, \"with average\", str(avg))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42106ed3",
   "metadata": {},
   "source": [
    "Effects of different hyperparameters on the F1 score on the test data:\n",
    "_Note that all other variables were constant wrt their initial settings from 2d) when constructed._\n",
    "**Learning Rate** - This is adjusted by changing the learning_rate parameter of the MLPClassifier. We used values of 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 0.75, 1, 2, and 5 as test learning rates. For very small learning rates eg 0.001, the accuracy on the test set was fairly low, sitting at about 90% across muliple reruns. Slight increases in learning rate to 0.01 and 0.05 showed increased performance on the unseen test set, with further increases at and beyond ~0.5 again decreasing the test accuracy.\n",
    "\n",
    "Generally, for very small learning rates, the amount of time required for convergence is much higher. It could be that with the number of epochs used in the default case (100), a learning rate of 0.001 was too low for convergence to be reached as each iteration takes a very small step, and thus the model is not fully developed at the point execution is terminated. Conversely, for larger learning rates (start to see the decrease in accuracy at0.1, but particularly those above ~0.5), we see a further decrease in accuracy. This could be due to the jumping behaviour that is often seen for large learning rates, where relative minima are completely overshot, and we never encounter convergence. Therefore, it appears that for this model, learning rates beyond about 0.5 are too large.\n",
    "Therefore, for this model, it appears the best learning rate to use is somewhere between 0.005 and 0.05, depending on whether we have more epochs than the baseline classifier (thus a smaller learning rate) or less (thus need a slightly larger learning rate).\n",
    "\n",
    "**Number of Epochs** - this is adjusted by changing the value of max_iter passed into the MLPClassifier. The values we tried were 5, 50, 100, 200, 500, 1000, and 5000.\n",
    "For low epoch numbers (5 and 50), we recieved convergence warnings when running our models, saying that the number of epochs was too low for MLPClassifier to have converged. Additionally, for the lower 5 epoch run, the accuracy was quite clearly lower compared to higher epoch counts. This suggests that for these low epoch counts, we are not reaching an optimal classifier before execution is terminated.\n",
    "\n",
    "For all runs of 100 epochs or higher, the accuracy seems fairly consistent. This suggests that with the baseline settings from 2d), 100 epochs is sufficient to have a well trained model. As the number of epochs increases, so does computation expense, so that is also an important consideration to make to ensure that the model will terminate in good time. It is also worth noting that the default value for this in MLPClassifier is 200 - which is twice as large as our default. \n",
    "\n",
    "For this model, any epoch count greater than 100 appears to be sufficient - this should be increased higher depending on the other hyperparameters we set.\n",
    "\n",
    "**Activation Function** - Scikit Learn has an activation function parameter, which sets the activation function of the hidden layers. The options were \"relu\", \"identity\", \"logistic\" (ie sigmoid), and \"tanh\". THe initial classifier we developed used the RelU function.\n",
    "Running this multiple times, there was some variation, but all four models stayed relatively similar. For this reason, it makes most sense to stick with Relu, which is relatively fast to train (so should stay high performing regardless of number of epochs). Note also that scikit learn by default uses sigmoid on the output layer.\n",
    "\n",
    "**Number of Hidden Layers** - This was the final parameter we decided to adjust, since the number of nodes was already adjusted for 2d). We used values of 1, 2, 3, 4, 5, 10, 20, and 40.\n",
    "The baseline classifier used 1 hidden layer, which is as low as possible, but had a decent accuracy score. As the number of hidden layers increased to 3 and above, the accuracy massively decreased. This could be due to overfitting on the training dataset, as the increased number of hidden layers makes the model more sensitive to noise and outliers in the training set. Also, as the number of hidden layers increases, the model is more susceptible to vanishing (very small) and exploding (very large) gradients during the back propogation algorithm, making weights disproportionate.\n",
    "\n",
    "Since the number of hidden layers has nothing to do wth the number of epochs or learning rate, we have decided to set it to 2, to help further train the model slightly without risking overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410a330b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
