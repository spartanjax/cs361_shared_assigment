{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ddbfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"../../train.csv\")\n",
    "X = df[\"Text\"].values\n",
    "y = df[\"Category\"].values\n",
    "m = 0.9 #proportion of data for training vs validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=m, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78d1cec",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae98f0",
   "metadata": {},
   "source": [
    "(d) NN. Consider a neural network with the following hyperparameters: the initial weights\n",
    "uniformly drawn in range [0,0.1] with learning rate 0.01.\n",
    "\n",
    "    ● Train a single hidden layer neural network using the hyperparameters on the training dataset, except for the number of hidden units (x) which should vary among 5, 20, and 40. Run the optimization for 100 epochs each time. Namely, the input layer consists of n features x = [x1, ..., xn]^T , the hidden layer has x nodes z = [z1, ..., zx]^T , and the output layer is a probability distribution y = [y1, y2]^T over two classes.\n",
    "\n",
    "    ● Plot the average training cross-entropy loss as shown below on the y-axis versus the number of hidden units on the x-axis. Explain the effect of numbers of hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ba117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector vocabulary - None\n",
      "\n",
      "features\n",
      " ['00' '000' '000th' ... 'zooms' 'zooropa' 'zorro']\n",
      "\n",
      "vector shape: (428, 13518)\n",
      "\n",
      "article vector\n",
      " [[0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[[9.41061006e-01 5.89389945e-02]\n",
      " [3.13270850e-05 9.99968673e-01]\n",
      " [7.02200449e-06 9.99992978e-01]\n",
      " [9.99973817e-01 2.61830141e-05]\n",
      " [9.99946559e-01 5.34409106e-05]\n",
      " [9.99459303e-01 5.40697294e-04]\n",
      " [9.99195568e-01 8.04432157e-04]\n",
      " [7.03442978e-05 9.99929656e-01]\n",
      " [9.93203958e-01 6.79604201e-03]\n",
      " [9.99482128e-01 5.17871599e-04]\n",
      " [9.99731486e-01 2.68513523e-04]\n",
      " [3.59020463e-04 9.99640980e-01]\n",
      " [9.84321854e-01 1.56781459e-02]\n",
      " [9.99284745e-01 7.15255372e-04]\n",
      " [9.99999505e-01 4.94509252e-07]\n",
      " [9.99332921e-01 6.67079195e-04]\n",
      " [9.38282034e-01 6.17179660e-02]\n",
      " [9.99711879e-01 2.88121450e-04]\n",
      " [5.74542828e-03 9.94254572e-01]\n",
      " [8.76703965e-06 9.99991233e-01]\n",
      " [9.99960973e-01 3.90271765e-05]\n",
      " [9.99944617e-01 5.53831362e-05]\n",
      " [2.23312223e-04 9.99776688e-01]\n",
      " [9.73332958e-01 2.66670418e-02]\n",
      " [6.26652237e-03 9.93733478e-01]\n",
      " [9.98442720e-01 1.55728029e-03]\n",
      " [1.30981437e-02 9.86901856e-01]\n",
      " [7.85027608e-01 2.14972392e-01]\n",
      " [9.98554569e-01 1.44543097e-03]\n",
      " [1.31226570e-04 9.99868773e-01]\n",
      " [9.95926225e-01 4.07377514e-03]\n",
      " [1.95298708e-02 9.80470129e-01]\n",
      " [9.43865730e-04 9.99056134e-01]\n",
      " [2.88533907e-04 9.99711466e-01]\n",
      " [9.86234212e-01 1.37657881e-02]\n",
      " [1.30981437e-02 9.86901856e-01]\n",
      " [9.95101672e-01 4.89832764e-03]\n",
      " [3.55244348e-04 9.99644756e-01]\n",
      " [3.07819621e-03 9.96921804e-01]\n",
      " [1.98196749e-02 9.80180325e-01]\n",
      " [7.48937337e-05 9.99925106e-01]\n",
      " [9.99975611e-01 2.43887413e-05]\n",
      " [9.99999243e-01 7.57371412e-07]\n",
      " [2.25244004e-06 9.99997748e-01]\n",
      " [9.94021255e-01 5.97874536e-03]\n",
      " [9.67960980e-01 3.20390199e-02]\n",
      " [4.66852700e-01 5.33147300e-01]\n",
      " [9.99296728e-01 7.03272111e-04]\n",
      " [9.99631236e-01 3.68763817e-04]\n",
      " [1.12145923e-02 9.88785408e-01]\n",
      " [8.83849551e-02 9.11615045e-01]\n",
      " [9.70979939e-01 2.90200612e-02]\n",
      " [9.99973528e-01 2.64723311e-05]\n",
      " [9.98575068e-01 1.42493241e-03]\n",
      " [9.89109457e-01 1.08905433e-02]\n",
      " [1.24974981e-05 9.99987503e-01]\n",
      " [9.92745457e-01 7.25454286e-03]\n",
      " [2.73306601e-05 9.99972669e-01]\n",
      " [2.93620738e-06 9.99997064e-01]\n",
      " [3.29091451e-05 9.99967091e-01]\n",
      " [7.30632021e-05 9.99926937e-01]\n",
      " [5.61087837e-05 9.99943891e-01]\n",
      " [9.47488708e-01 5.25112923e-02]\n",
      " [1.69882919e-04 9.99830117e-01]\n",
      " [9.26690274e-01 7.33097256e-02]\n",
      " [9.93727117e-01 6.27288302e-03]\n",
      " [2.76235437e-07 9.99999724e-01]\n",
      " [9.87966592e-01 1.20334082e-02]\n",
      " [9.99505132e-01 4.94868323e-04]\n",
      " [7.02200449e-06 9.99992978e-01]\n",
      " [9.99465528e-01 5.34471994e-04]\n",
      " [9.99999999e-01 1.46476762e-09]\n",
      " [3.80036802e-03 9.96199632e-01]\n",
      " [9.93244115e-01 6.75588450e-03]\n",
      " [9.82296385e-01 1.77036146e-02]\n",
      " [9.93263169e-01 6.73683084e-03]\n",
      " [4.24037146e-06 9.99995760e-01]\n",
      " [9.95126222e-01 4.87377788e-03]\n",
      " [9.94538779e-01 5.46122119e-03]\n",
      " [9.99945677e-01 5.43234344e-05]\n",
      " [9.99983102e-01 1.68982847e-05]\n",
      " [9.99365117e-01 6.34882627e-04]\n",
      " [5.76828791e-02 9.42317121e-01]\n",
      " [5.43330187e-05 9.99945667e-01]\n",
      " [1.76206679e-04 9.99823793e-01]\n",
      " [1.50722253e-03 9.98492777e-01]\n",
      " [9.99534961e-01 4.65038838e-04]\n",
      " [9.88517000e-01 1.14829997e-02]\n",
      " [9.82715735e-01 1.72842654e-02]\n",
      " [2.67299601e-03 9.97327004e-01]\n",
      " [9.93278509e-01 6.72149052e-03]\n",
      " [9.99149565e-01 8.50434854e-04]\n",
      " [9.99973230e-01 2.67695769e-05]\n",
      " [1.20235640e-05 9.99987976e-01]\n",
      " [9.85441124e-01 1.45588756e-02]\n",
      " [4.73207748e-07 9.99999527e-01]\n",
      " [9.52224715e-06 9.99990478e-01]\n",
      " [3.33793060e-04 9.99666207e-01]\n",
      " [9.99708384e-01 2.91615649e-04]\n",
      " [3.01074778e-04 9.99698925e-01]\n",
      " [1.83620128e-05 9.99981638e-01]\n",
      " [7.81529483e-01 2.18470517e-01]\n",
      " [9.89863522e-01 1.01364776e-02]\n",
      " [9.99986131e-01 1.38689707e-05]\n",
      " [5.21805042e-04 9.99478195e-01]\n",
      " [9.86531908e-03 9.90134681e-01]]\n",
      "['entertainment' 'tech']\n",
      "0               tech\n",
      "1      entertainment\n",
      "2      entertainment\n",
      "3      entertainment\n",
      "4      entertainment\n",
      "           ...      \n",
      "423    entertainment\n",
      "424    entertainment\n",
      "425             tech\n",
      "426    entertainment\n",
      "427             tech\n",
      "Name: Category, Length: 428, dtype: object\n",
      "probability 1 0.9410610055074548\n",
      "loss1 -0.06074731099077313\n",
      "probability 2 0.9999686729150343\n",
      "loss2 -3.13275756690455e-05\n",
      "probability 2 0.9999929779955087\n",
      "loss2 -7.022029145708354e-06\n",
      "probability 2 2.618301405876367e-05\n",
      "loss2 -10.550399675814251\n",
      "probability 2 5.344091063797514e-05\n",
      "loss2 -9.836933988499798\n",
      "probability 1 0.999459302706348\n",
      "loss1 -0.0005408435231466777\n",
      "probability 2 0.0008044321566373512\n",
      "loss2 -7.125373924935714\n",
      "probability 1 7.034429780716778e-05\n",
      "loss1 -9.562108832313461\n",
      "probability 1 0.9932039579897792\n",
      "loss1 -0.006819240267685737\n",
      "probability 1 0.9994821284005292\n",
      "loss1 -0.000518005741281734\n",
      "probability 2 0.0002685135229195272\n",
      "loss2 -8.222609280592637\n",
      "probability 2 0.9996409795368408\n",
      "loss2 -0.0003590849264352007\n",
      "probability 1 0.9843218541035149\n",
      "loss1 -0.015802347907767297\n",
      "probability 1 0.9992847446284124\n",
      "loss1 -0.0007155112887488826\n",
      "probability 1 0.999999505490748\n",
      "loss1 -4.945093742612e-07\n",
      "probability 2 0.0006670791954352336\n",
      "loss2 -7.312601785310991\n",
      "probability 2 0.06171796598658545\n",
      "loss2 -2.785180207529485\n",
      "probability 1 0.9997118785497521\n",
      "loss1 -0.0002881629652073961\n",
      "probability 1 0.005745428279128606\n",
      "loss1 -5.1593508223088\n",
      "probability 1 8.767039645585228e-06\n",
      "loss1 -11.644511363229892\n",
      "probability 1 0.9999609728235429\n",
      "loss1 -3.902793803716202e-05\n",
      "probability 2 5.538313618059447e-05\n",
      "loss2 -9.801235411622748\n",
      "probability 1 0.00022331222255722594\n",
      "loss1 -8.40693966457839\n",
      "probability 2 0.026667041791775686\n",
      "loss2 -3.624326865883719\n",
      "probability 1 0.006266522372015637\n",
      "loss1 -5.072533723821625\n",
      "probability 1 0.9984427197138366\n",
      "loss1 -0.001558494107445232\n",
      "probability 1 0.013098143676669727\n",
      "loss1 -4.335284762887015\n",
      "probability 1 0.785027607633598\n",
      "loss1 -0.24203639285814452\n",
      "probability 2 0.0014454309741005094\n",
      "loss2 -6.5393477499133486\n",
      "probability 1 0.00013122657010111727\n",
      "loss1 -8.938585185943271\n",
      "probability 2 0.004073775137600784\n",
      "loss2 -5.503185157202322\n",
      "probability 1 0.019529870837760366\n",
      "loss1 -3.935810147645354\n",
      "probability 1 0.0009438657304691178\n",
      "loss1 -6.9655266366076285\n",
      "probability 1 0.00028853390714622673\n",
      "loss1 -8.150697949819085\n",
      "probability 2 0.01376578809260839\n",
      "loss2 -4.2855688886586325\n",
      "probability 1 0.013098143676669727\n",
      "loss1 -4.335284762887015\n",
      "probability 2 0.004898327638549587\n",
      "loss2 -5.318861430376248\n",
      "probability 1 0.00035524434814870887\n",
      "loss1 -7.9427047006176075\n",
      "probability 1 0.003078196212263351\n",
      "loss1 -5.78341149892293\n",
      "probability 1 0.01981967491117953\n",
      "loss1 -3.92108015227437\n",
      "probability 2 0.9999251062663131\n",
      "loss2 -7.489653836256256e-05\n",
      "probability 2 2.438874132035998e-05\n",
      "loss2 -10.621388953449404\n",
      "probability 1 0.9999992426285881\n",
      "loss1 -7.573716987249892e-07\n",
      "probability 2 0.9999977475599593\n",
      "loss2 -2.252442577453338e-06\n",
      "probability 2 0.005978745359531228\n",
      "loss2 -5.119544539130432\n",
      "probability 2 0.032039019868830954\n",
      "loss2 -3.4408007481103455\n",
      "probability 1 0.4668527002370306\n",
      "loss1 -0.7617414881189867\n",
      "probability 2 0.0007032721113428843\n",
      "loss2 -7.259766669435001\n",
      "probability 1 0.9996312361827638\n",
      "loss1 -0.00036883182733290224\n",
      "probability 2 0.9887854076782119\n",
      "loss2 -0.011277949994591648\n",
      "probability 1 0.08838495509112343\n",
      "loss1 -2.4260535150996025\n",
      "probability 2 0.02902006123668393\n",
      "loss2 -3.53976792137381\n",
      "probability 1 0.9999735276689473\n",
      "loss1 -2.6472681451024475e-05\n",
      "probability 2 0.0014249324087748068\n",
      "loss2 -6.553630898825209\n",
      "probability 2 0.010890543291899089\n",
      "loss2 -4.519860454218408\n",
      "probability 1 1.2497498148467123e-05\n",
      "loss1 -11.289982081810956\n",
      "probability 2 0.007254542864572538\n",
      "loss2 -4.926127404338619\n",
      "probability 2 0.9999726693398727\n",
      "loss2 -2.7331033616574903e-05\n",
      "probability 2 0.9999970637926205\n",
      "loss2 -2.936211690121205e-06\n",
      "probability 1 3.2909145108339644e-05\n",
      "loss1 -10.321759971999732\n",
      "probability 2 0.9999269367978796\n",
      "loss2 -7.306587136618444e-05\n",
      "probability 2 0.9999438912163261\n",
      "loss2 -5.6110357830588344e-05\n",
      "probability 2 0.05251129231643422\n",
      "loss2 -2.946727040771867\n",
      "probability 1 0.00016988291916641973\n",
      "loss1 -8.68040106897\n",
      "probability 1 0.9266902744116879\n",
      "loss1 -0.07613588530623545\n",
      "probability 2 0.006272883017069202\n",
      "loss2 -5.071519218683345\n",
      "probability 1 2.7623543685795227e-07\n",
      "loss1 -15.102012302760333\n",
      "probability 1 0.9879665918009299\n",
      "loss1 -0.012106395772644399\n",
      "probability 2 0.000494868322857285\n",
      "loss2 -7.6112188452134255\n",
      "probability 2 0.9999929779955087\n",
      "loss2 -7.022029145708354e-06\n",
      "probability 2 0.0005344719940872341\n",
      "loss2 -7.534231225341077\n",
      "probability 2 1.4647676232115761e-09\n",
      "loss2 -20.341569226026905\n",
      "probability 2 0.9961996319786893\n",
      "loss2 -0.0038076077681484313\n",
      "probability 2 0.006755884500743657\n",
      "loss2 -4.9973413759873875\n",
      "probability 1 0.9822963853500655\n",
      "loss1 -0.017862198090160857\n",
      "probability 2 0.006736830835344232\n",
      "loss2 -5.00016566710895\n",
      "probability 2 0.9999957596285353\n",
      "loss2 -4.240380455137042e-06\n",
      "probability 1 0.9951262221240037\n",
      "loss1 -0.004885693463103871\n",
      "probability 2 0.005461221185215826\n",
      "loss2 -5.21008285393762\n",
      "probability 2 5.43234343803224e-05\n",
      "loss2 -9.820554851743331\n",
      "probability 1 0.999983101715312\n",
      "loss1 -1.6898427465603987e-05\n",
      "probability 1 0.9993651173725823\n",
      "loss1 -0.0006350842507356416\n",
      "probability 1 0.05768287913476877\n",
      "loss1 -2.852794871599857\n",
      "probability 1 5.433301868018603e-05\n",
      "loss1 -9.82037843700461\n",
      "probability 2 0.9998237933206158\n",
      "loss2 -0.00017622220560506677\n",
      "probability 2 0.998492777472111\n",
      "loss2 -0.0015083595303841246\n",
      "probability 2 0.00046503883849905136\n",
      "loss2 -7.6733896322109505\n",
      "probability 1 0.9885170003215635\n",
      "loss1 -0.011549438419640873\n",
      "probability 2 0.017284265379309598\n",
      "loss2 -4.0579587069855645\n",
      "probability 2 0.9973270039942724\n",
      "loss2 -0.0026765748384438644\n",
      "probability 1 0.9932785094837588\n",
      "loss1 -0.006744181468793411\n",
      "probability 1 0.9991495651457201\n",
      "loss1 -0.0008507966791541569\n",
      "probability 1 0.9999732304230654\n",
      "loss1 -2.6769935246112462e-05\n",
      "probability 1 1.202356404250704e-05\n",
      "loss1 -11.328642163447439\n",
      "probability 2 0.014558875632601675\n",
      "loss2 -4.229554462231048\n",
      "probability 2 0.9999995267922516\n",
      "loss2 -4.7320786039322616e-07\n",
      "probability 1 9.522247146898088e-06\n",
      "loss1 -11.561879692172965\n",
      "probability 1 0.00033379305957048633\n",
      "loss1 -8.00498933913219\n",
      "probability 1 0.9997083843505488\n",
      "loss1 -0.0002916581775628498\n",
      "probability 1 0.00030107477765650703\n",
      "loss1 -8.108151893316693\n",
      "probability 1 1.83620127681694e-05\n",
      "loss1 -10.905226550884692\n",
      "probability 2 0.21847051738791387\n",
      "loss2 -1.5211042054137094\n",
      "probability 2 0.010136477577149517\n",
      "loss2 -4.591614720148492\n",
      "probability 1 0.9999861310293037\n",
      "loss1 -1.3869066871392408e-05\n",
      "probability 2 0.9994781949580243\n",
      "loss2 -0.0005219412296043129\n",
      "probability 1 0.00986531908397148\n",
      "loss1 -4.618729794975253\n",
      "0.07745466274790044\n",
      "[[9.44033892e-01 5.59661079e-02]\n",
      " [3.24821952e-05 9.99967518e-01]\n",
      " [2.49483040e-06 9.99997505e-01]\n",
      " [9.99868739e-01 1.31261337e-04]\n",
      " [9.99873488e-01 1.26511869e-04]\n",
      " [9.99097079e-01 9.02921165e-04]\n",
      " [9.99099273e-01 9.00726808e-04]\n",
      " [6.83468249e-05 9.99931653e-01]\n",
      " [9.95256006e-01 4.74399392e-03]\n",
      " [9.98607924e-01 1.39207600e-03]\n",
      " [9.99203246e-01 7.96754292e-04]\n",
      " [1.97129353e-04 9.99802871e-01]\n",
      " [9.81043771e-01 1.89562290e-02]\n",
      " [9.98935918e-01 1.06408216e-03]\n",
      " [9.99996244e-01 3.75565675e-06]\n",
      " [9.98878565e-01 1.12143522e-03]\n",
      " [9.26273152e-01 7.37268477e-02]\n",
      " [9.99625872e-01 3.74127645e-04]\n",
      " [1.12764217e-03 9.98872358e-01]\n",
      " [4.62407219e-06 9.99995376e-01]\n",
      " [9.99842904e-01 1.57096033e-04]\n",
      " [9.99818672e-01 1.81327562e-04]\n",
      " [6.42939411e-04 9.99357061e-01]\n",
      " [9.76974209e-01 2.30257914e-02]\n",
      " [6.47795382e-03 9.93522046e-01]\n",
      " [9.96426547e-01 3.57345267e-03]\n",
      " [1.70113441e-02 9.82988656e-01]\n",
      " [7.65486985e-01 2.34513015e-01]\n",
      " [9.98312158e-01 1.68784192e-03]\n",
      " [9.30149703e-05 9.99906985e-01]\n",
      " [9.93983194e-01 6.01680642e-03]\n",
      " [2.85705455e-02 9.71429455e-01]\n",
      " [4.54264332e-04 9.99545736e-01]\n",
      " [1.40038013e-04 9.99859962e-01]\n",
      " [9.88868189e-01 1.11318114e-02]\n",
      " [1.70113441e-02 9.82988656e-01]\n",
      " [9.93514709e-01 6.48529078e-03]\n",
      " [1.41684469e-04 9.99858316e-01]\n",
      " [2.36365282e-03 9.97636347e-01]\n",
      " [1.67787452e-02 9.83221255e-01]\n",
      " [1.54650452e-05 9.99984535e-01]\n",
      " [9.99988245e-01 1.17551384e-05]\n",
      " [9.99996053e-01 3.94680093e-06]\n",
      " [2.36685559e-07 9.99999763e-01]\n",
      " [9.94557297e-01 5.44270289e-03]\n",
      " [9.73821583e-01 2.61784173e-02]\n",
      " [4.80639270e-01 5.19360730e-01]\n",
      " [9.98554543e-01 1.44545670e-03]\n",
      " [9.99183985e-01 8.16015345e-04]\n",
      " [1.53934310e-02 9.84606569e-01]\n",
      " [1.08854005e-01 8.91145995e-01]\n",
      " [9.77671363e-01 2.23286367e-02]\n",
      " [9.99939751e-01 6.02492827e-05]\n",
      " [9.98195991e-01 1.80400862e-03]\n",
      " [9.88606661e-01 1.13933387e-02]\n",
      " [2.17956324e-06 9.99997820e-01]\n",
      " [9.93455717e-01 6.54428266e-03]\n",
      " [1.33364448e-05 9.99986664e-01]\n",
      " [3.10862982e-06 9.99996891e-01]\n",
      " [1.76489304e-05 9.99982351e-01]\n",
      " [1.50792132e-04 9.99849208e-01]\n",
      " [2.07131977e-05 9.99979287e-01]\n",
      " [8.65762127e-01 1.34237873e-01]\n",
      " [3.26519467e-04 9.99673481e-01]\n",
      " [8.55452466e-01 1.44547534e-01]\n",
      " [9.92515511e-01 7.48448914e-03]\n",
      " [8.14564880e-08 9.99999919e-01]\n",
      " [9.88575453e-01 1.14245471e-02]\n",
      " [9.98672660e-01 1.32734038e-03]\n",
      " [2.49483040e-06 9.99997505e-01]\n",
      " [9.99210565e-01 7.89434909e-04]\n",
      " [9.99999946e-01 5.39338548e-08]\n",
      " [3.73333543e-03 9.96266665e-01]\n",
      " [9.94165435e-01 5.83456512e-03]\n",
      " [9.85890653e-01 1.41093466e-02]\n",
      " [9.93363577e-01 6.63642269e-03]\n",
      " [1.22276596e-05 9.99987772e-01]\n",
      " [9.97552520e-01 2.44748040e-03]\n",
      " [9.97299734e-01 2.70026553e-03]\n",
      " [9.99840039e-01 1.59960837e-04]\n",
      " [9.99970905e-01 2.90947983e-05]\n",
      " [9.98927952e-01 1.07204802e-03]\n",
      " [1.13268277e-01 8.86731723e-01]\n",
      " [1.43477588e-05 9.99985652e-01]\n",
      " [1.77791194e-04 9.99822209e-01]\n",
      " [6.48235620e-04 9.99351764e-01]\n",
      " [9.99440174e-01 5.59825874e-04]\n",
      " [9.89698788e-01 1.03012122e-02]\n",
      " [9.91630881e-01 8.36911933e-03]\n",
      " [2.78451154e-03 9.97215488e-01]\n",
      " [9.95466196e-01 4.53380369e-03]\n",
      " [9.99324012e-01 6.75987555e-04]\n",
      " [9.99931100e-01 6.88995488e-05]\n",
      " [1.92377232e-05 9.99980762e-01]\n",
      " [9.81225171e-01 1.87748292e-02]\n",
      " [6.90930130e-08 9.99999931e-01]\n",
      " [9.50333586e-07 9.99999050e-01]\n",
      " [6.39695078e-04 9.99360305e-01]\n",
      " [9.99557964e-01 4.42035730e-04]\n",
      " [3.94681255e-04 9.99605319e-01]\n",
      " [3.73997938e-06 9.99996260e-01]\n",
      " [8.52424641e-01 1.47575359e-01]\n",
      " [9.93191595e-01 6.80840526e-03]\n",
      " [9.99956196e-01 4.38036998e-05]\n",
      " [3.24859204e-04 9.99675141e-01]\n",
      " [2.60338033e-03 9.97396620e-01]]\n",
      "['entertainment' 'tech']\n",
      "0               tech\n",
      "1      entertainment\n",
      "2      entertainment\n",
      "3      entertainment\n",
      "4      entertainment\n",
      "           ...      \n",
      "423    entertainment\n",
      "424    entertainment\n",
      "425             tech\n",
      "426    entertainment\n",
      "427             tech\n",
      "Name: Category, Length: 428, dtype: object\n",
      "probability 1 0.9440338920981477\n",
      "loss1 -0.057593210834778044\n",
      "probability 2 0.999967517804804\n",
      "loss2 -3.2482722753952924e-05\n",
      "probability 2 0.9999975051696045\n",
      "loss2 -2.494833507588668e-06\n",
      "probability 2 0.00013126133665357131\n",
      "loss2 -8.938320285719204\n",
      "probability 2 0.00012651186915347442\n",
      "loss2 -8.975174426898011\n",
      "probability 1 0.9990970788354462\n",
      "loss1 -0.0009033290434086321\n",
      "probability 2 0.0009007268076712759\n",
      "loss2 -7.012308556465191\n",
      "probability 1 6.83468249449426e-05\n",
      "loss1 -9.590915448763663\n",
      "probability 1 0.9952560060766419\n",
      "loss1 -0.004755282378254656\n",
      "probability 1 0.9986079240038551\n",
      "loss1 -0.0013930458340976491\n",
      "probability 2 0.000796754292378782\n",
      "loss2 -7.1349642173217145\n",
      "probability 2 0.9998028706473191\n",
      "loss2 -0.0001971487852256054\n",
      "probability 1 0.981043770958957\n",
      "loss1 -0.01913820169762607\n",
      "probability 1 0.9989359178359013\n",
      "loss1 -0.0010646487014551991\n",
      "probability 1 0.9999962443432531\n",
      "loss1 -3.7556637994252507e-06\n",
      "probability 2 0.0011214352152835983\n",
      "loss2 -6.793145971803971\n",
      "probability 2 0.07372684766944818\n",
      "loss2 -2.6073882629543035\n",
      "probability 1 0.999625872354544\n",
      "loss1 -0.00037419764866415033\n",
      "probability 1 0.001127642171205978\n",
      "loss1 -6.787626400340656\n",
      "probability 1 4.62407219481431e-06\n",
      "loss1 -12.284234813650702\n",
      "probability 1 0.9998429039672444\n",
      "loss1 -0.0001571083736298382\n",
      "probability 2 0.00018132756230892206\n",
      "loss2 -8.61520542578515\n",
      "probability 1 0.0006429394113897091\n",
      "loss1 -7.349460066176176\n",
      "probability 2 0.02302579143935171\n",
      "loss2 -3.771140324386915\n",
      "probability 1 0.006477953823123106\n",
      "loss1 -5.039350586519736\n",
      "probability 1 0.996426547333287\n",
      "loss1 -0.003579852700053461\n",
      "probability 1 0.017011344072731194\n",
      "loss1 -4.073874859074546\n",
      "probability 1 0.7654869854198393\n",
      "loss1 -0.2672430654399768\n",
      "probability 2 0.0016878419200248593\n",
      "loss2 -6.384304536468111\n",
      "probability 1 9.301497033420425e-05\n",
      "loss1 -9.282750106429983\n",
      "probability 2 0.006016806419016988\n",
      "loss2 -5.113198655603663\n",
      "probability 1 0.02857054546187321\n",
      "loss1 -3.5553789708015393\n",
      "probability 1 0.0004542643318149464\n",
      "loss1 -7.696831300684993\n",
      "probability 1 0.00014003801294915075\n",
      "loss1 -8.87359665114478\n",
      "probability 2 0.011131811406576623\n",
      "loss2 -4.497948377019977\n",
      "probability 1 0.017011344072731194\n",
      "loss1 -4.073874859074546\n",
      "probability 2 0.006485290782963856\n",
      "loss2 -5.038218622903044\n",
      "probability 1 0.00014168446866524853\n",
      "loss1 -8.86190802443621\n",
      "probability 1 0.0023636528235836396\n",
      "loss1 -6.047547049959426\n",
      "probability 1 0.016778745243083426\n",
      "loss1 -4.087642357670869\n",
      "probability 2 0.9999845349547711\n",
      "loss2 -1.5465164813913383e-05\n",
      "probability 2 1.1755138436569935e-05\n",
      "loss2 -11.351220099217263\n",
      "probability 1 0.9999960531990665\n",
      "loss1 -3.946808722157922e-06\n",
      "probability 2 0.999999763314441\n",
      "loss2 -2.3668558705080373e-07\n",
      "probability 2 0.0054427028947972265\n",
      "loss2 -5.2134794858449345\n",
      "probability 2 0.026178417334618648\n",
      "loss2 -3.6428199735508757\n",
      "probability 1 0.48063927007790763\n",
      "loss1 -0.7326382484937392\n",
      "probability 2 0.0014454567042318065\n",
      "loss2 -6.5393299490617345\n",
      "probability 1 0.9991839846550571\n",
      "loss1 -0.0008163484666984735\n",
      "probability 2 0.9846065689860513\n",
      "loss2 -0.015513139949588636\n",
      "probability 1 0.10885400547932833\n",
      "loss1 -2.2177476938221776\n",
      "probability 2 0.022328636685850754\n",
      "loss2 -3.8018852680234057\n",
      "probability 1 0.9999397507173085\n",
      "loss1 -6.0251097752485925e-05\n",
      "probability 2 0.0018040086174134589\n",
      "loss2 -6.317744080516385\n",
      "probability 2 0.011393338655732454\n",
      "loss2 -4.474726422812608\n",
      "probability 1 2.1795632417775934e-06\n",
      "loss1 -13.036386049044022\n",
      "probability 2 0.0065442826638557414\n",
      "loss2 -5.029163486260057\n",
      "probability 2 0.9999866635551952\n",
      "loss2 -1.3336533736005236e-05\n",
      "probability 2 0.9999968913701838\n",
      "loss2 -3.108634648028295e-06\n",
      "probability 1 1.7648930394575935e-05\n",
      "loss1 -10.94483537729516\n",
      "probability 2 0.9998492078676041\n",
      "loss2 -0.00015080350267252554\n",
      "probability 2 0.9999792868023387\n",
      "loss2 -2.0713412182522732e-05\n",
      "probability 2 0.13423787306805876\n",
      "loss2 -2.0081418806374307\n",
      "probability 1 0.00032651946690354094\n",
      "loss1 -8.02702098804342\n",
      "probability 1 0.8554524660630898\n",
      "loss1 -0.15612474994856496\n",
      "probability 2 0.007484489142912847\n",
      "loss2 -4.894922514219583\n",
      "probability 1 8.145648799917637e-08\n",
      "loss1 -16.323196848850042\n",
      "probability 1 0.9885754528847386\n",
      "loss1 -0.011490308596857637\n",
      "probability 2 0.0013273403795519979\n",
      "loss2 -6.624578053469848\n",
      "probability 2 0.9999975051696045\n",
      "loss2 -2.494833507588668e-06\n",
      "probability 2 0.0007894349085783971\n",
      "loss2 -7.144193174053285\n",
      "probability 2 5.393385478110323e-08\n",
      "loss2 -16.735507452660418\n",
      "probability 2 0.9962666645659171\n",
      "loss2 -0.003740321724345057\n",
      "probability 2 0.005834565119644726\n",
      "loss2 -5.143955545644957\n",
      "probability 1 0.9858906533527921\n",
      "loss1 -0.014209829765795772\n",
      "probability 2 0.006636422691706575\n",
      "loss2 -5.015182211921404\n",
      "probability 2 0.9999877723403918\n",
      "loss2 -1.2227734366616339e-05\n",
      "probability 1 0.9975525196025347\n",
      "loss1 -0.0024504803735347345\n",
      "probability 2 0.00270026553175964\n",
      "loss2 -5.914405165711252\n",
      "probability 2 0.00015996083670075014\n",
      "loss2 -8.740581543311976\n",
      "probability 1 0.9999709052017232\n",
      "loss1 -2.909522153869402e-05\n",
      "probability 1 0.9989279519786607\n",
      "loss1 -0.0010726230758467759\n",
      "probability 1 0.11326827711830534\n",
      "loss1 -2.1779961402824277\n",
      "probability 1 1.434775876485439e-05\n",
      "loss1 -11.151916811590418\n",
      "probability 2 0.9998222088058925\n",
      "loss2 -0.00017780700083545628\n",
      "probability 2 0.9993517643804417\n",
      "loss2 -0.0006484458151099271\n",
      "probability 2 0.0005598258740133619\n",
      "loss2 -7.487884761848568\n",
      "probability 1 0.9896987878363763\n",
      "loss1 -0.01035463685908745\n",
      "probability 2 0.008369119331576443\n",
      "loss2 -4.783206617271382\n",
      "probability 2 0.9972154884550054\n",
      "loss2 -0.0027883955089037698\n",
      "probability 1 0.9954661963104285\n",
      "loss1 -0.0045441125482143865\n",
      "probability 1 0.9993240124451193\n",
      "loss1 -0.0006762161374863878\n",
      "probability 1 0.9999311004511765\n",
      "loss1 -6.890192250644292e-05\n",
      "probability 1 1.9237723213549884e-05\n",
      "loss1 -10.858637455823063\n",
      "probability 2 0.018774829213738336\n",
      "loss2 -3.9752381778453043\n",
      "probability 2 0.999999930906987\n",
      "loss2 -6.909301536456136e-08\n",
      "probability 1 9.503335863803031e-07\n",
      "loss1 -13.866452770430081\n",
      "probability 1 0.0006396950775152677\n",
      "loss1 -7.354518936527209\n",
      "probability 1 0.9995579642704531\n",
      "loss1 -0.0004421334561401866\n",
      "probability 1 0.0003946812547608314\n",
      "loss1 -7.8374320687951196\n",
      "probability 1 3.739979381300884e-06\n",
      "loss1 -12.496430459574185\n",
      "probability 2 0.14757535949250744\n",
      "loss2 -1.9134163218596558\n",
      "probability 2 0.006808405261797221\n",
      "loss2 -4.989597362191729\n",
      "probability 1 0.9999561963002372\n",
      "loss1 -4.380465917283036e-05\n",
      "probability 2 0.9996751407960672\n",
      "loss2 -0.0003249119821145884\n",
      "probability 1 0.002603380325608251\n",
      "loss1 -5.950944553151738\n",
      "0.05308244796758192\n",
      "[[9.57873157e-01 4.21268428e-02]\n",
      " [1.52897268e-04 9.99847103e-01]\n",
      " [3.94395437e-06 9.99996056e-01]\n",
      " [9.99951656e-01 4.83443385e-05]\n",
      " [9.99929693e-01 7.03068798e-05]\n",
      " [9.98777890e-01 1.22210987e-03]\n",
      " [9.99557340e-01 4.42659755e-04]\n",
      " [3.68266102e-04 9.99631734e-01]\n",
      " [9.96916581e-01 3.08341873e-03]\n",
      " [9.99231945e-01 7.68054591e-04]\n",
      " [9.99613018e-01 3.86982332e-04]\n",
      " [6.52603010e-04 9.99347397e-01]\n",
      " [9.86842034e-01 1.31579656e-02]\n",
      " [9.98937928e-01 1.06207208e-03]\n",
      " [9.99998130e-01 1.87037364e-06]\n",
      " [9.98993179e-01 1.00682125e-03]\n",
      " [9.57547202e-01 4.24527981e-02]\n",
      " [9.99854697e-01 1.45302855e-04]\n",
      " [8.97753797e-03 9.91022462e-01]\n",
      " [7.05297575e-06 9.99992947e-01]\n",
      " [9.99922876e-01 7.71239203e-05]\n",
      " [9.99864212e-01 1.35787791e-04]\n",
      " [2.73459855e-04 9.99726540e-01]\n",
      " [9.65568104e-01 3.44318961e-02]\n",
      " [4.70619929e-03 9.95293801e-01]\n",
      " [9.98409510e-01 1.59049037e-03]\n",
      " [1.50719847e-02 9.84928015e-01]\n",
      " [7.80090982e-01 2.19909018e-01]\n",
      " [9.98025171e-01 1.97482930e-03]\n",
      " [2.32378680e-04 9.99767621e-01]\n",
      " [9.96543332e-01 3.45666771e-03]\n",
      " [1.99260468e-02 9.80073953e-01]\n",
      " [1.09887792e-03 9.98901122e-01]\n",
      " [3.41349038e-04 9.99658651e-01]\n",
      " [9.90563064e-01 9.43693556e-03]\n",
      " [1.50719847e-02 9.84928015e-01]\n",
      " [9.92978781e-01 7.02121933e-03]\n",
      " [1.98170919e-04 9.99801829e-01]\n",
      " [3.83421282e-03 9.96165787e-01]\n",
      " [1.37258306e-02 9.86274169e-01]\n",
      " [1.44900244e-05 9.99985510e-01]\n",
      " [9.99987638e-01 1.23617009e-05]\n",
      " [9.99997477e-01 2.52328838e-06]\n",
      " [3.37093551e-06 9.99996629e-01]\n",
      " [9.94977346e-01 5.02265429e-03]\n",
      " [9.56271965e-01 4.37280345e-02]\n",
      " [6.37016556e-01 3.62983444e-01]\n",
      " [9.98754837e-01 1.24516334e-03]\n",
      " [9.99314371e-01 6.85629225e-04]\n",
      " [1.67514959e-02 9.83248504e-01]\n",
      " [1.31714533e-01 8.68285467e-01]\n",
      " [9.76183374e-01 2.38166262e-02]\n",
      " [9.99965886e-01 3.41136070e-05]\n",
      " [9.97687396e-01 2.31260381e-03]\n",
      " [9.93897573e-01 6.10242746e-03]\n",
      " [8.31220964e-06 9.99991688e-01]\n",
      " [9.94962259e-01 5.03774133e-03]\n",
      " [4.99685127e-05 9.99950031e-01]\n",
      " [4.72375039e-06 9.99995276e-01]\n",
      " [1.25039219e-04 9.99874961e-01]\n",
      " [1.44128053e-04 9.99855872e-01]\n",
      " [2.58559546e-05 9.99974144e-01]\n",
      " [8.84152327e-01 1.15847673e-01]\n",
      " [7.34909827e-04 9.99265090e-01]\n",
      " [9.14617948e-01 8.53820522e-02]\n",
      " [9.93074662e-01 6.92533824e-03]\n",
      " [1.76579593e-07 9.99999823e-01]\n",
      " [9.90015778e-01 9.98422201e-03]\n",
      " [9.98899143e-01 1.10085700e-03]\n",
      " [3.94395437e-06 9.99996056e-01]\n",
      " [9.99135011e-01 8.64989352e-04]\n",
      " [9.99999990e-01 9.65851331e-09]\n",
      " [1.49573936e-03 9.98504261e-01]\n",
      " [9.97478697e-01 2.52130305e-03]\n",
      " [9.91480930e-01 8.51907049e-03]\n",
      " [9.94484116e-01 5.51588421e-03]\n",
      " [2.07907996e-05 9.99979209e-01]\n",
      " [9.97484742e-01 2.51525766e-03]\n",
      " [9.96827062e-01 3.17293766e-03]\n",
      " [9.99930219e-01 6.97814636e-05]\n",
      " [9.99968299e-01 3.17010788e-05]\n",
      " [9.99718649e-01 2.81350997e-04]\n",
      " [6.79726027e-02 9.32027397e-01]\n",
      " [1.23681220e-05 9.99987632e-01]\n",
      " [1.30659497e-04 9.99869341e-01]\n",
      " [1.07810105e-03 9.98921899e-01]\n",
      " [9.99645320e-01 3.54679652e-04]\n",
      " [9.87242953e-01 1.27570470e-02]\n",
      " [9.92202688e-01 7.79731246e-03]\n",
      " [2.97176897e-03 9.97028231e-01]\n",
      " [9.94335009e-01 5.66499084e-03]\n",
      " [9.98780433e-01 1.21956734e-03]\n",
      " [9.99926180e-01 7.38198280e-05]\n",
      " [1.88323088e-05 9.99981168e-01]\n",
      " [9.72869982e-01 2.71300184e-02]\n",
      " [2.58015345e-07 9.99999742e-01]\n",
      " [1.81404527e-05 9.99981860e-01]\n",
      " [2.32900312e-04 9.99767100e-01]\n",
      " [9.99571854e-01 4.28146387e-04]\n",
      " [4.75421246e-04 9.99524579e-01]\n",
      " [6.40428276e-06 9.99993596e-01]\n",
      " [8.61774031e-01 1.38225969e-01]\n",
      " [9.92291585e-01 7.70841512e-03]\n",
      " [9.99968166e-01 3.18344988e-05]\n",
      " [4.74264469e-04 9.99525736e-01]\n",
      " [5.24740691e-03 9.94752593e-01]]\n",
      "['entertainment' 'tech']\n",
      "0               tech\n",
      "1      entertainment\n",
      "2      entertainment\n",
      "3      entertainment\n",
      "4      entertainment\n",
      "           ...      \n",
      "423    entertainment\n",
      "424    entertainment\n",
      "425             tech\n",
      "426    entertainment\n",
      "427             tech\n",
      "Name: Category, Length: 428, dtype: object\n",
      "probability 1 0.957873157204484\n",
      "loss1 -0.043039913530575485\n",
      "probability 2 0.9998471027323187\n",
      "loss2 -0.00015290895766013134\n",
      "probability 2 0.9999960560456302\n",
      "loss2 -3.943962147164627e-06\n",
      "probability 2 4.8344338460399976e-05\n",
      "loss2 -9.937161437845626\n",
      "probability 2 7.030687977426117e-05\n",
      "loss2 -9.562640900859671\n",
      "probability 1 0.9987778901293699\n",
      "loss1 -0.0012228572558846934\n",
      "probability 2 0.00044265975519956963\n",
      "loss2 -7.722709129971877\n",
      "probability 1 0.0003682661015339539\n",
      "loss1 -7.906704779113188\n",
      "probability 1 0.996916581270784\n",
      "loss1 -0.0030881822592378648\n",
      "probability 1 0.9992319454089049\n",
      "loss1 -0.0007683496961367277\n",
      "probability 2 0.0003869823321626874\n",
      "loss2 -7.857131519303026\n",
      "probability 2 0.9993473969900761\n",
      "loss2 -0.0006528160479593676\n",
      "probability 1 0.9868420344312593\n",
      "loss1 -0.013245298526347139\n",
      "probability 1 0.9989379279214797\n",
      "loss1 -0.0010626364767266968\n",
      "probability 1 0.9999981296263649\n",
      "loss1 -1.8703753842331937e-06\n",
      "probability 2 0.0010068212486176184\n",
      "loss2 -6.900957189822904\n",
      "probability 2 0.04245279812890717\n",
      "loss2 -3.1593624525218202\n",
      "probability 1 0.9998546971454455\n",
      "loss1 -0.000145313412036968\n",
      "probability 1 0.008977537970472516\n",
      "loss1 -4.713029602357802\n",
      "probability 1 7.052975754429092e-06\n",
      "loss1 -11.862060937369394\n",
      "probability 1 0.9999228760796974\n",
      "loss1 -7.712689450503248e-05\n",
      "probability 2 0.00013578779052976478\n",
      "loss2 -8.904417254615312\n",
      "probability 1 0.0002734598554389178\n",
      "loss1 -8.204355727720507\n",
      "probability 2 0.03443189608473688\n",
      "loss2 -3.3687719324324763\n",
      "probability 1 0.004706199294374658\n",
      "loss1 -5.35887464057777\n",
      "probability 1 0.9984095096323139\n",
      "loss1 -0.0015917565402258685\n",
      "probability 1 0.015071984746953593\n",
      "loss1 -4.194917573160744\n",
      "probability 1 0.7800909819481318\n",
      "loss1 -0.24834472257758058\n",
      "probability 2 0.001974829300274606\n",
      "loss2 -6.227273314605066\n",
      "probability 1 0.00023237867953029845\n",
      "loss1 -8.367142277598116\n",
      "probability 2 0.00345666770688271\n",
      "loss2 -5.667450244478467\n",
      "probability 1 0.019926046825764487\n",
      "loss1 -3.9157275173790715\n",
      "probability 1 0.0010988779246318137\n",
      "loss1 -6.8134656883183125\n",
      "probability 1 0.00034134903788496374\n",
      "loss1 -7.982605032662727\n",
      "probability 2 0.009436935560617065\n",
      "loss2 -4.663123974342109\n",
      "probability 1 0.015071984746953593\n",
      "loss1 -4.194917573160744\n",
      "probability 2 0.007021219330252385\n",
      "loss2 -4.958818382259287\n",
      "probability 1 0.00019817091857232683\n",
      "loss1 -8.526380674521501\n",
      "probability 1 0.003834212818454086\n",
      "loss1 -5.563791127695221\n",
      "probability 1 0.013725830564619401\n",
      "loss1 -4.288475778691117\n",
      "probability 2 0.9999855099756008\n",
      "loss2 -1.449012938065619e-05\n",
      "probability 2 1.2361700862486268e-05\n",
      "loss2 -11.300907505168633\n",
      "probability 1 0.9999974767116214\n",
      "loss1 -2.5232915620727704e-06\n",
      "probability 2 0.9999966290644945\n",
      "loss2 -3.370941187124791e-06\n",
      "probability 2 0.005022654285408225\n",
      "loss2 -5.293796742900164\n",
      "probability 2 0.04372803451496216\n",
      "loss2 -3.129765860486405\n",
      "probability 1 0.6370165564720744\n",
      "loss1 -0.45095963242580434\n",
      "probability 2 0.0012451633416063156\n",
      "loss2 -6.688488559593819\n",
      "probability 1 0.9993143707747995\n",
      "loss1 -0.000685864376408274\n",
      "probability 2 0.9832485041080079\n",
      "loss2 -0.016893389046399174\n",
      "probability 1 0.13171453257646293\n",
      "loss1 -2.02711833025784\n",
      "probability 2 0.02381662616013094\n",
      "loss2 -3.737371364040088\n",
      "probability 1 0.9999658863930347\n",
      "loss1 -3.411418884758165e-05\n",
      "probability 2 0.002312603813054747\n",
      "loss2 -6.0693811974535326\n",
      "probability 2 0.0061024274623050065\n",
      "loss2 -5.099068642321675\n",
      "probability 1 8.312209638816626e-06\n",
      "loss1 -11.697785083261069\n",
      "probability 2 0.005037741334388131\n",
      "loss2 -5.290797445286022\n",
      "probability 2 0.9999500314872856\n",
      "loss2 -4.996976118215881e-05\n",
      "probability 2 0.9999952762496096\n",
      "loss2 -4.723761547344749e-06\n",
      "probability 1 0.0001250392188532068\n",
      "loss1 -8.986883119045816\n",
      "probability 2 0.9998558719470575\n",
      "loss2 -0.00014413844038842092\n",
      "probability 2 0.9999741440454024\n",
      "loss2 -2.5856288868542735e-05\n",
      "probability 2 0.11584767254869756\n",
      "loss2 -2.155479118519654\n",
      "probability 1 0.0007349098268574217\n",
      "loss1 -7.215762750825528\n",
      "probability 1 0.9146179477539862\n",
      "loss1 -0.08924884434041641\n",
      "probability 2 0.006925338241194834\n",
      "loss2 -4.9725683846077\n",
      "probability 1 1.7657959283656766e-07\n",
      "loss1 -15.549494111305469\n",
      "probability 1 0.9900157779862923\n",
      "loss1 -0.010034398620608644\n",
      "probability 2 0.001100856999212633\n",
      "loss2 -6.811666312317735\n",
      "probability 2 0.9999960560456302\n",
      "loss2 -3.943962147164627e-06\n",
      "probability 2 0.0008649893521060157\n",
      "loss2 -7.0527933608121876\n",
      "probability 2 9.658513305626856e-09\n",
      "loss2 -18.45542610267515\n",
      "probability 2 0.9985042606410899\n",
      "loss2 -0.0014968590937185831\n",
      "probability 2 0.0025213030535609195\n",
      "loss2 -5.9829794263546585\n",
      "probability 1 0.9914809295141831\n",
      "loss1 -0.008555565181862658\n",
      "probability 2 0.005515884208588209\n",
      "loss2 -5.200123311187983\n",
      "probability 2 0.9999792092004302\n",
      "loss2 -2.0791015701516613e-05\n",
      "probability 1 0.9974847423441238\n",
      "loss1 -0.002518426230717208\n",
      "probability 2 0.0031729376587812273\n",
      "loss2 -5.753097413979945\n",
      "probability 2 6.978146361221703e-05\n",
      "loss2 -9.570142147759274\n",
      "probability 1 0.9999682989211841\n",
      "loss1 -3.170158130571207e-05\n",
      "probability 1 0.9997186490030144\n",
      "loss1 -0.0002813905836026458\n",
      "probability 1 0.06797260268511918\n",
      "loss1 -2.6886505566821475\n",
      "probability 1 1.236812199656434e-05\n",
      "loss1 -11.300388202280054\n",
      "probability 2 0.9998693405026907\n",
      "loss2 -0.0001306680340050092\n",
      "probability 2 0.9989218989450752\n",
      "loss2 -0.0010786826238980906\n",
      "probability 2 0.00035467965191042256\n",
      "loss2 -7.94429556487163\n",
      "probability 1 0.9872429529889258\n",
      "loss1 -0.012839116861693917\n",
      "probability 2 0.007797312463068711\n",
      "loss2 -4.853976160676486\n",
      "probability 2 0.997028231026031\n",
      "loss2 -0.0029761934472354885\n",
      "probability 1 0.9943350091600245\n",
      "loss1 -0.005681097759756737\n",
      "probability 1 0.9987804326609122\n",
      "loss1 -0.0012203116165275863\n",
      "probability 1 0.9999261801719578\n",
      "loss1 -7.382255285982483e-05\n",
      "probability 1 1.8832308785965424e-05\n",
      "loss1 -10.879936610704961\n",
      "probability 2 0.02713001842796008\n",
      "loss2 -3.607114473064934\n",
      "probability 2 0.9999997419846547\n",
      "loss2 -2.5801537859730036e-07\n",
      "probability 1 1.814045274051157e-05\n",
      "loss1 -10.917366155456893\n",
      "probability 1 0.00023290031188449856\n",
      "loss1 -8.364900041940059\n",
      "probability 1 0.9995718536127316\n",
      "loss1 -0.00042823806810234673\n",
      "probability 1 0.00047542124613830783\n",
      "loss1 -7.651309312959162\n",
      "probability 1 6.404282756444779e-06\n",
      "loss1 -11.958543610705716\n",
      "probability 2 0.1382259686674506\n",
      "loss2 -1.978865478879273\n",
      "probability 2 0.0077084151211447354\n",
      "loss2 -4.865442673997379\n",
      "probability 1 0.9999681655011687\n",
      "loss1 -3.183500555967468e-05\n",
      "probability 2 0.9995257355314621\n",
      "loss2 -0.0004743769675019052\n",
      "probability 1 0.005247406914429709\n",
      "loss1 -5.2500212454588\n",
      "0.051689653419591634\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    data = make_df(\"../../train.csv\")\n",
    "    y = data[\"Category\"]\n",
    "    test_data = make_df(\"../../test.csv\")\n",
    "    y_test = test_data[\"Category\"]       \n",
    "    vector, vector_test = vectorize(data, test_data)\n",
    " #   make_classifier(vector, data, vector_test)\n",
    "\n",
    "    cels = []\n",
    "    num_nods = [5, 20, 40]\n",
    "    for num in num_nods:\n",
    "        probs, clf = make_classifier(vector, y, vector_test, num)\n",
    "        cel = calc_cels(probs, y, clf)\n",
    "        cels.append(cel)\n",
    "\n",
    "    plt.plot(num_nods, cels, marker = \"*\", color = \"#F543BA\", markersize = 10)\n",
    "    plt.title(\"Cross entropy loss\")\n",
    "    plt.show()\n",
    "\n",
    "    ms = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    test_f1s = []\n",
    "    train_f1s = []\n",
    "\n",
    "    for m in ms:\n",
    "        train_f1, test_f1 = train_on_m(y, vector, vector_test, m, y_test)\n",
    "        test_f1s.append(test_f1)\n",
    "        train_f1s.append(train_f1)\n",
    "\n",
    "    \n",
    "    plt.plot(ms, test_f1s, marker = \"o\", color = \"#BA52FF\", markersize = 7)\n",
    "    plt.title(\"Test f1s\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(ms, train_f1s, marker = \"x\", color = \"#F431CB\", markersize = 8)\n",
    "    plt.title(\"Train f1s\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def vectorize(data, test_data):\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(data[\"Text\"])\n",
    "    print(f'vector vocabulary - {vectorizer.vocabulary}\\n')\n",
    "\n",
    "    vector = vectorizer.transform(data[\"Text\"])\n",
    "    test_vector = vectorizer.transform(test_data[\"Text\"])\n",
    "    \n",
    "    print(f'features\\n {vectorizer.get_feature_names_out()}\\n')\n",
    "\n",
    "    print(f'vector shape: {vector.shape}\\n')\n",
    "    print(f'article vector\\n {vector.toarray()}')\n",
    "\n",
    "    return vector, test_vector\n",
    "\n",
    "def make_df(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def calc_cels(probs, train_y, clf):\n",
    "    total = 0\n",
    "    print(probs)\n",
    "    print(clf.classes_)\n",
    "    print(train_y)\n",
    "    for ind in range(len(probs)):\n",
    "        loss1 = 0\n",
    "        loss2 = 0\n",
    "        if train_y[ind] == \"tech\":\n",
    "            loss1 = math.log(probs[ind][1])\n",
    "            print(\"probability 1\", probs[ind][1])\n",
    "            print(\"loss1\", loss1)\n",
    "        else:\n",
    "            loss1 = 0\n",
    "        if train_y[ind] == \"entertainment\":\n",
    "            loss2 = math.log(probs[ind][0])\n",
    "            print(\"probability 2\", probs[ind][0])\n",
    "            print(\"loss2\", loss2)\n",
    "        else:\n",
    "            loss2 = 0\n",
    "        cel = - (loss1 + loss2)\n",
    "        total += cel\n",
    "\n",
    "    print(np.mean(clf.loss_curve_))\n",
    "\n",
    "    avg_cel = total / len(probs)\n",
    "\n",
    "    return avg_cel\n",
    "\n",
    "# passes in training xs, training ys, testing xs\n",
    "''' THIS IS THE CLASSIFIER FUNCTION YOU SHOULD MIRROR FOR YOUR MODEL FOR IT TO WORK WITH TASK 3\n",
    "    IGNORE num_nodes that is specific to the NN classifier. \n",
    "'''\n",
    "def make_classifier(vector, y, vector_test, num_nodes = 5): \n",
    "\n",
    "    clf = MLPClassifier(activation = 'relu', solver = 'sgd', learning_rate_init = 0.01,\n",
    "                        max_iter = 100, hidden_layer_sizes = (num_nodes,)).fit(vector, y)\n",
    "\n",
    "\n",
    "    probs = clf.predict_proba(vector_test)\n",
    "\n",
    "    return probs, clf\n",
    "        \n",
    "def train_on_m(train_y, vector, vector_test, m, y_test):\n",
    "    \n",
    "    sub_vector = vector[0:round(vector.shape[0]*m)]\n",
    "    \n",
    "    probs, clf = make_classifier(sub_vector, train_y[0:round(len(train_y)*m)], vector_test)\n",
    "\n",
    "    preds_test = clf.predict(vector_test)\n",
    "\n",
    "    preds_train = clf.predict(sub_vector)\n",
    "\n",
    "    train_f1 = calc_f1(preds_train, train_y)\n",
    "    test_f1 = calc_f1(preds_test, y_test)\n",
    "\n",
    "    return train_f1, test_f1\n",
    "\n",
    "def calc_f1(preds, actual):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "\n",
    "# tech is positive\n",
    "\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] == \"tech\" and actual[i] == \"tech\":\n",
    "            tp += 1\n",
    "        elif preds[i] == \"tech\" and actual[i] == \"entertainment\":\n",
    "            fp += 1\n",
    "        elif preds[i] == \"entertainment\" and actual[i] == \"entertainment\":\n",
    "            tn += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2*((precision*recall) / (precision+recall))\n",
    "\n",
    "    return f1\n",
    "\n",
    "main()\n",
    "\n",
    "# hyperparams; #hidden layers, #neurons per layer, #activation function\n",
    "\n",
    "# adj initial weight, adj learning rate, adj # epoch,\n",
    "#adj # hidden units (1layer has x units)\n",
    "\n",
    "#act function relu\n",
    "#solver sgd\n",
    "#alpha leave default\n",
    "#learning_rate_init: set to 0.01\n",
    "#max_iter: 100 (#epochs)\n",
    "#\n",
    "\n",
    "# got a warning.warn from warnings which told us there was a warning about convergance, stopped training bc max iter = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6639512b",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72002d5",
   "metadata": {},
   "source": [
    "(a) We explore how the size of the training data set affects the test and train accuracy. For each\n",
    "value of m in [0.1, 0.3, 0.5, 0.7, 0.9], train your classifier on the first m portion of the training\n",
    "examples (that is, use the data given by XTrain[0:mN] and yTrain[0:mN]). Please report two\n",
    "plots: (i) training and (ii) testing accuracy for each such value of m with the x-axis referring to m\n",
    "and the y-axis referring to the classification accuracy in 𝐹1 measure as shown below. In total,\n",
    "there should be four curves for training accuracy and four curves for testing accuracy. Explain\n",
    "the general trend of the two plots in terms of training and testing accuracy if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753e48e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_m(train_y, vector, vector_test, m, y_test):\n",
    "    \n",
    "    sub_vector = vector[0:round(vector.shape[0]*m)]\n",
    "    \n",
    "    probs, clf = make_classifier(sub_vector, train_y[0:round(len(train_y)*m)], vector_test)\n",
    "\n",
    "    preds_test = clf.predict(vector_test)\n",
    "\n",
    "    preds_train = clf.predict(sub_vector)\n",
    "\n",
    "    train_f1 = calc_f1(preds_train, train_y)\n",
    "    test_f1 = calc_f1(preds_test, y_test)\n",
    "\n",
    "    return train_f1, test_f1\n",
    "\n",
    "def calc_f1(preds, actual):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "\n",
    "# tech is positive\n",
    "\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] == \"tech\" and actual[i] == \"tech\":\n",
    "            tp += 1\n",
    "        elif preds[i] == \"tech\" and actual[i] == \"entertainment\":\n",
    "            fp += 1\n",
    "        elif preds[i] == \"entertainment\" and actual[i] == \"entertainment\":\n",
    "            tn += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2*((precision*recall) / (precision+recall))\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d8471a",
   "metadata": {},
   "source": [
    "(b) Let’s use 5-fold cross-validation to assess model performance. Investigate the impact of key\n",
    "hyperparameters of your choices for each classifier using a testing dataset. E.g., for SVM, the\n",
    "classification accuracy may be significantly affected by the kernels and hyperparameter\n",
    "combination. List hyperparameters for each classifier and demonstrate how these\n",
    "hyperparameters impact on the testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c523aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector vocabulary - None\n",
      "\n",
      "features\n",
      " ['00' '000' '000th' ... 'zooms' 'zooropa' 'zorro']\n",
      "\n",
      "vector shape: (428, 13518)\n",
      "\n",
      "article vector\n",
      " [[0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Scores for normal classifier: [1.         0.98837052 0.94178963 1.         0.97646733] with average 0.9813254964344751\n",
      "LEARNING RATE\n",
      "Score for learning rate 0.001: [1.         0.95058824 0.65       0.78571429 1.        ] with average 0.8772605042016807\n",
      "Score for learning rate 0.005: [0.95206972 0.95058824 0.95058824 0.90277778 1.        ] with average 0.9512047930283224\n",
      "Score for learning rate 0.01: [1.         1.         1.         0.84444444 1.        ] with average 0.968888888888889\n",
      "Score for learning rate 0.05: [0.95206972 0.95058824 0.89903846 0.78571429 1.        ] with average 0.9174821398644928\n",
      "Score for learning rate 0.1: [1.         0.95058824 0.95058824 0.78571429 1.        ] with average 0.9373781512605042\n",
      "Score for learning rate 0.5: [1.         0.95058824 0.90277778 0.78571429 1.        ] with average 0.9278160597572362\n",
      "Score for learning rate 0.75: [1.         0.95058824 0.80909091 0.90454545 0.85714286] with average 0.9042734912146676\n",
      "Score for learning rate 1: [1.         0.90454545 0.95194508 0.84444444 1.        ] with average 0.9401869958162866\n",
      "Score for learning rate 2: [0.95368421 0.95058824 0.80909091 0.95058824 1.        ] with average 0.9327903180410921\n",
      "Score for learning rate 5: [1.         0.95194508 0.36363636 0.78571429 0.90454545] with average 0.8011682367975274\n",
      "MAX ITERATIONS\n",
      "Score for max iterations 5: [0.90598291 0.3872679  0.85176471 0.85176471 1.        ] with average 0.7993560444513792\n",
      "Score for max iterations 50: [1.         0.95058824 0.72148541 0.84444444 1.        ] with average 0.9033036181758292\n",
      "Score for max iterations 100: [1.         0.95058824 0.78571429 0.89903846 0.89903846] with average 0.9068758888170653\n",
      "Score for max iterations 200: [0.95206972 0.95058824 0.78571429 0.84444444 1.        ] with average 0.9065633364456893\n",
      "Score for max iterations 500: [0.95206972 0.95058824 0.95058824 0.84444444 1.        ] with average 0.9395381263616558\n",
      "Score for max iterations 1000: [1.         0.95058824 1.         0.89903846 1.        ] with average 0.9699253393665159\n",
      "Score for max iterations 5000: [1.         0.95058824 1.         0.84444444 1.        ] with average 0.9590065359477125\n",
      "ACTIVATION FUNCTION\n",
      "Score for activation function relu: [1.         1.         1.         0.84444444 1.        ] with average 0.968888888888889\n",
      "Score for activation function identity: [1.         0.95058824 0.95058824 0.84444444 1.        ] with average 0.9491241830065359\n",
      "Score for activation function logistic: [1.         0.95058824 1.         0.84444444 1.        ] with average 0.9590065359477125\n",
      "Score for activation function tanh: [1.         0.95058824 1.         0.84444444 1.        ] with average 0.9590065359477125\n",
      "HIDDEN LAYERS\n",
      "Score for number of layers =  1: [1.         0.95058824 0.95058824 0.84444444 1.        ] with average 0.9491241830065359\n",
      "Score for number of layers =  2: [1.         0.95058824 1.         0.84444444 0.84444444] with average 0.9278954248366013\n",
      "Score for number of layers =  3: [0.84827586 0.95058824 0.84444444 0.90277778 0.95194508] with average 0.8996062799353677\n",
      "Score for number of layers =  4: [1.         0.78571429 0.78571429 0.78571429 0.85714286] with average 0.8428571428571427\n",
      "Score for number of layers =  5: [1.         0.90277778 0.80909091 0.95194508 0.85714286] with average 0.9041913248206154\n",
      "Score for number of layers =  10: [1.         0.36363636 0.36363636 0.36363636 0.36363636] with average 0.490909090909091\n",
      "Score for number of layers =  20: [0.37142857 0.36363636 0.36363636 0.36363636 0.36363636] with average 0.3651948051948052\n",
      "Score for number of layers =  40: [0.37142857 0.36363636 0.36363636 0.36363636 0.36363636] with average 0.3651948051948052\n",
      "Learning rate = 0.001, Max iter = 100\n",
      "Score for number of layers =  40: [1.         0.59615385 0.90454545 0.95058824 0.95194508] with average 0.8806465232169904\n",
      "Learning rate = 0.005, Max iter = 100\n",
      "Score for number of layers =  40: [1.         0.80909091 1.         0.84444444 0.84444444] with average 0.8995959595959595\n",
      "Learning rate = 0.01, Max iter = 100\n",
      "Score for number of layers =  40: [1.         0.95058824 0.95058824 0.78571429 1.        ] with average 0.9373781512605042\n",
      "Learning rate = 0.05, Max iter = 100\n",
      "Score for number of layers =  40: [1.         0.95058824 1.         0.90277778 0.89903846] with average 0.9504808949220713\n",
      "Learning rate = 0.1, Max iter = 100\n",
      "Score for number of layers =  40: [1.         0.95058824 1.         0.78571429 0.95058824] with average 0.9373781512605042\n",
      "Learning rate = 0.001, Max iter = 200\n",
      "Score for number of layers =  40: [0.29032258 1.         0.36363636 0.89903846 0.3       ] with average 0.5705994811639973\n",
      "Learning rate = 0.005, Max iter = 200\n",
      "Score for number of layers =  40: [1.         0.95058824 0.78571429 0.84444444 0.89903846] with average 0.8959570853982619\n",
      "Learning rate = 0.01, Max iter = 200\n",
      "Score for number of layers =  40: [1.         0.95058824 1.         0.78571429 0.85714286] with average 0.9186890756302521\n",
      "Learning rate = 0.05, Max iter = 200\n",
      "Score for number of layers =  40: [0.95206972 0.95058824 0.95194508 0.85176471 1.        ] with average 0.9412735476087206\n",
      "Learning rate = 0.1, Max iter = 200\n",
      "Score for number of layers =  40: [1.         0.95058824 0.95058824 0.84444444 0.85714286] with average 0.9205527544351074\n",
      "Learning rate = 0.001, Max iter = 500\n",
      "Score for number of layers =  40: [0.37142857 0.90454545 0.65       0.84444444 1.        ] with average 0.7540836940836941\n",
      "Learning rate = 0.005, Max iter = 500\n",
      "Score for number of layers =  40: [0.95206972 0.95058824 1.         0.89903846 1.        ] with average 0.9603392827216357\n",
      "Learning rate = 0.01, Max iter = 500\n",
      "Score for number of layers =  40: [1.         0.84444444 1.         0.84444444 0.85714286] with average 0.9092063492063491\n",
      "Learning rate = 0.05, Max iter = 500\n",
      "Score for number of layers =  40: [1.         0.90277778 1.         0.89903846 1.        ] with average 0.9603632478632479\n",
      "Learning rate = 0.1, Max iter = 500\n",
      "Score for number of layers =  40: [0.95206972 0.95058824 0.95194508 0.84444444 1.        ] with average 0.9398094953211389\n",
      "Learning rate = 0.001, Max iter = 1000\n",
      "Score for number of layers =  40: [0.90178571 0.95058824 0.78571429 0.78571429 0.84444444] with average 0.8536493930905695\n",
      "Learning rate = 0.005, Max iter = 1000\n",
      "Score for number of layers =  40: [1.         0.95058824 0.72148541 0.84444444 1.        ] with average 0.9033036181758292\n",
      "Learning rate = 0.01, Max iter = 1000\n",
      "Score for number of layers =  40: [1.         0.95058824 0.95194508 0.89903846 1.        ] with average 0.9603143553848226\n",
      "Learning rate = 0.05, Max iter = 1000\n",
      "Score for number of layers =  40: [0.95206972 0.95058824 1.         0.78571429 0.85714286] with average 0.9091030189853718\n",
      "Learning rate = 0.1, Max iter = 1000\n",
      "Score for number of layers =  40: [1.         0.95058824 1.         0.85176471 0.95194508] with average 0.9508596042536009\n",
      "Learning rate = 0.001, Max iter = 5000\n",
      "Score for number of layers =  40: [0.95206972 0.72148541 0.95058824 0.84444444 0.95194508] with average 0.8841065775492556\n",
      "Learning rate = 0.005, Max iter = 5000\n",
      "Score for number of layers =  40: [1.         0.84444444 1.         0.84444444 1.        ] with average 0.9377777777777778\n",
      "Learning rate = 0.01, Max iter = 5000\n",
      "Score for number of layers =  40: [0.95206972 0.95058824 0.78571429 0.95194508 0.85714286] with average 0.8994920350036786\n",
      "Learning rate = 0.05, Max iter = 5000\n",
      "Score for number of layers =  40: [1.         0.95058824 1.         0.78571429 0.85714286] with average 0.9186890756302521\n",
      "Learning rate = 0.1, Max iter = 5000\n",
      "Score for number of layers =  40: [1.         0.95058824 0.89903846 0.89903846 1.        ] with average 0.9497330316742083\n"
     ]
    }
   ],
   "source": [
    "#use 5-fold CV to assess performance, experiment w diff hyperparams\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def vectorize(data, test_data):\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(data[\"Text\"])\n",
    "    print(f'vector vocabulary - {vectorizer.vocabulary}\\n')\n",
    "\n",
    "    vector = vectorizer.transform(data[\"Text\"])\n",
    "    test_vector = vectorizer.transform(test_data[\"Text\"])\n",
    "    \n",
    "    print(f'features\\n {vectorizer.get_feature_names_out()}\\n')\n",
    "\n",
    "    print(f'vector shape: {vector.shape}\\n')\n",
    "    print(f'article vector\\n {vector.toarray()}')\n",
    "\n",
    "    return vector, test_vector\n",
    "\n",
    "def make_df(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    data = make_df(\"../../train.csv\")\n",
    "    y = data[\"Category\"]\n",
    "    test_data = make_df(\"../../test.csv\")\n",
    "    y_test = test_data[\"Category\"]       \n",
    "    vector, vector_test = vectorize(data, test_data)\n",
    "    probs, model = make_classifier(vector, y, vector_test)\n",
    "\n",
    "    cross_validate(model, vector, y, vector_test, y_test)\n",
    "\n",
    "    # passes in training xs, training ys, testing xs\n",
    "def make_classifier(vector, y, vector_test, num_nodes = 5): # discuss num nodes in report\n",
    "\n",
    "    clf = MLPClassifier(activation = 'relu', solver = 'sgd', learning_rate_init = 0.01,\n",
    "                        max_iter = 100, hidden_layer_sizes = (num_nodes,)).fit(vector, y)\n",
    "\n",
    "\n",
    "    probs = clf.predict_proba(vector_test)\n",
    "\n",
    "    return probs, clf\n",
    "\n",
    "def cross_validate(clf, train_descriptions, train_y, test_descriptions, test_y):\n",
    "    # use kfold validation (k = 5).\n",
    "    # hyperparameters that we used:\n",
    "    \n",
    "    scores = cross_val_score(clf, train_descriptions, train_y, cv=5, scoring=\"f1_macro\")\n",
    "\n",
    "    print(\"Scores for normal classifier:\", scores, \"with average\", str(sum(scores) / len(scores)))\n",
    "\n",
    "    # LEARNING RATE\n",
    "    # very low learning rate had bad accuracy suggesting that it does not converge to minima with that epoch count\n",
    "    # marginal change, but incr learning rate caused slightly lower avg perf, so better to keep small (not too small for computations sake.\n",
    "    print(\"LEARNING RATE\")\n",
    "    learning_rates = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 0.75, 1, 2, 5]\n",
    "    for learning_rate in learning_rates:\n",
    "        # retrain clf\n",
    "        clf = MLPClassifier(activation = 'relu', solver = 'sgd', learning_rate_init = learning_rate,\n",
    "                        max_iter = 100, hidden_layer_sizes = (5,)).fit(train_descriptions, train_y)\n",
    "        get_scores(clf, test_descriptions, test_y, \"learning rate\", learning_rate)\n",
    "\n",
    "    # MAX ITER\n",
    "    # for very low values of max_iter_val, we get a convergence warning\n",
    "    # always for 5 and 50, sometimes for 100, never for bigger than that\n",
    "    # really large epoch values don't give much noticeable change to the accuracy\n",
    "    # therefore suggesting there has been convergence before that many epochs and we are running unnecessarily\n",
    "    # so we will use the default from scikit learn of 200 epochs.\n",
    "    print(\"MAX ITERATIONS\")\n",
    "    max_iter_values = [5, 50, 100, 200, 500, 1000, 5000]\n",
    "    for max_iter_val in max_iter_values:\n",
    "        # retrain clf\n",
    "        clf = MLPClassifier(activation = 'relu', solver = 'sgd', learning_rate_init = 0.01,\n",
    "                        max_iter = max_iter_val, hidden_layer_sizes = (5,)).fit(train_descriptions, train_y)\n",
    "        get_scores(clf, test_descriptions, test_y, \"max iterations\", max_iter_val)\n",
    "\n",
    "    # ACTIVATION FUNCTION\n",
    "    # doesn't seem like there's much change beyond natural variation\n",
    "    # all between 97 and 99 which we have seen similar scores from relu alone\n",
    "    # might be worth looking at sigmoid vs relu for this context to justify continuing with relu\n",
    "    print(\"ACTIVATION FUNCTION\")\n",
    "    activation_functions = [\"relu\", \"identity\", \"logistic\", \"tanh\"]\n",
    "    for function in activation_functions:\n",
    "        # retrain clf\n",
    "        clf = MLPClassifier(activation = function, solver = 'sgd', learning_rate_init = 0.01,\n",
    "                        max_iter = 100, hidden_layer_sizes = (5,)).fit(train_descriptions, train_y)\n",
    "        get_scores(clf, test_descriptions, test_y, \"activation function\", function)\n",
    "\n",
    "    # HIDDEN LAYERS\n",
    "    # increasing number of hidden layers for this decreased accuracy, and also gave us warning from warnings.warn convergence warning\n",
    "    # this meant that the optimal number of hidden layers for 100 epoch is 1\n",
    "    # but may need fine tuning alongside other epoch numbers\n",
    "    print(\"HIDDEN LAYERS\")\n",
    "    num_layers = [1, 2, 3, 4, 5, 10, 20, 40]\n",
    "    for num in num_layers:\n",
    "        # retrain clf\n",
    "        sizes = (5,) * num\n",
    "        clf = MLPClassifier(activation = 'relu', solver = 'sgd', learning_rate_init = 0.01,\n",
    "                        max_iter = 100, hidden_layer_sizes = sizes).fit(train_descriptions, train_y)\n",
    "        get_scores(clf, test_descriptions, test_y, \"number of layers = \", num)\n",
    "\n",
    "\n",
    "    # EPOCHS & LEARNING RATE TOGETHER\n",
    "    # learning rate of 0.5 was best for any epoch size\n",
    "    # epoch of size 500 & 1000 with 0.5 learning rate tied best w score of 0.9603\n",
    "    # landed on epoch size 500 for computation sake\n",
    "\n",
    "    # taking subset of the better epochs and learning rates\n",
    "    max_iter_values = [100, 200, 500, 1000, 5000]\n",
    "    learning_rates = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "    for iter_val in max_iter_values:\n",
    "        for learning_rate in learning_rates:\n",
    "        # retrain clf for each combo of learning rate and max iter\n",
    "            clf = MLPClassifier(activation = 'relu', solver = 'sgd', learning_rate_init = learning_rate,\n",
    "                            max_iter = iter_val, hidden_layer_sizes = 2).fit(train_descriptions, train_y)\n",
    "            print(f\"Learning rate = {learning_rate}, Max iter = {iter_val}\")\n",
    "            get_scores(clf, test_descriptions, test_y, \"number of layers = \", num) # change this text\n",
    "\n",
    "\n",
    "def get_scores(clf, test_descriptions, test_y, hyperparam, value):\n",
    "        scores = cross_val_score(clf, test_descriptions, test_y, cv=5, scoring=\"f1_macro\")\n",
    "        avg = sum(scores) / len(scores)\n",
    "        print(f\"Score for {hyperparam} {value}:\", scores, \"with average\", str(avg))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42106ed3",
   "metadata": {},
   "source": [
    "Effects of different hyperparameters on the F1 score on the test data:\n",
    "_Note that all other variables were constant wrt their initial settings from 2d) when constructed._\n",
    "**Learning Rate** - This is adjusted by changing the learning_rate parameter of the MLPClassifier. We used values of 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 0.75, 1, 2, and 5 as test learning rates. For very small learning rates eg 0.001, the accuracy on the test set was fairly low, sitting at about 90% across muliple reruns. Slight increases in learning rate to 0.01 and 0.05 showed increased performance on the unseen test set, with further increases at and beyond ~0.5 again decreasing the test accuracy.\n",
    "\n",
    "Generally, for very small learning rates, the amount of time required for convergence is much higher. It could be that with the number of epochs used in the default case (100), a learning rate of 0.001 was too low for convergence to be reached as each iteration takes a very small step, and thus the model is not fully developed at the point execution is terminated. Conversely, for larger learning rates (start to see the decrease in accuracy at0.1, but particularly those above ~0.5), we see a further decrease in accuracy. This could be due to the jumping behaviour that is often seen for large learning rates, where relative minima are completely overshot, and we never encounter convergence. Therefore, it appears that for this model, learning rates beyond about 0.5 are too large.\n",
    "Therefore, for this model, it appears the best learning rate to use is somewhere between 0.005 and 0.05, depending on whether we have more epochs than the baseline classifier (thus a smaller learning rate) or less (thus need a slightly larger learning rate).\n",
    "\n",
    "**Number of Epochs** - this is adjusted by changing the value of max_iter passed into the MLPClassifier. The values we tried were 5, 50, 100, 200, 500, 1000, and 5000.\n",
    "For low epoch numbers (5 and 50), we recieved convergence warnings when running our models, saying that the number of epochs was too low for MLPClassifier to have converged. Additionally, for the lower 5 epoch run, the accuracy was quite clearly lower compared to higher epoch counts. This suggests that for these low epoch counts, we are not reaching an optimal classifier before execution is terminated.\n",
    "\n",
    "For all runs of 100 epochs or higher, the accuracy seems fairly consistent. This suggests that with the baseline settings from 2d), 100 epochs is sufficient to have a well trained model. As the number of epochs increases, so does computation expense, so that is also an important consideration to make to ensure that the model will terminate in good time. It is also worth noting that the default value for this in MLPClassifier is 200 - which is twice as large as our default. \n",
    "\n",
    "For this model, any epoch count greater than 100 appears to be sufficient - this should be increased higher depending on the other hyperparameters we set.\n",
    "\n",
    "**Activation Function** - Scikit Learn has an activation function parameter, which sets the activation function of the hidden layers. The options were \"relu\", \"identity\", \"logistic\" (ie sigmoid), and \"tanh\". THe initial classifier we developed used the RelU function.\n",
    "Running this multiple times, there was some variation, but all four models stayed relatively similar. For this reason, it makes most sense to stick with Relu, which is relatively fast to train (so should stay high performing regardless of number of epochs). Note also that scikit learn by default uses sigmoid on the output layer.\n",
    "\n",
    "**Number of Hidden Layers** - This was the final parameter we decided to adjust, since the number of nodes was already adjusted for 2d). We used values of 1, 2, 3, 4, 5, 10, 20, and 40.\n",
    "The baseline classifier used 1 hidden layer, which is as low as possible, but had a decent accuracy score. As the number of hidden layers increased to 3 and above, the accuracy massively decreased. This could be due to overfitting on the training dataset, as the increased number of hidden layers makes the model more sensitive to noise and outliers in the training set. Also, as the number of hidden layers increases, the model is more susceptible to vanishing (very small) and exploding (very large) gradients during the back propogation algorithm, making weights disproportionate.\n",
    "\n",
    "Since the number of hidden layers has nothing to do wth the number of epochs or learning rate, we have decided to set it to 2, to help further train the model slightly without risking overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410a330b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
