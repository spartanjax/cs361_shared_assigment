{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1 - Representation and Preprocessing Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my Naive Bayes Classifier, I have decided to represent the text in each abstract as a counter, where each key represents a words, and maps to an integer value representing the number of times that word appears in abstracts from that given class. This will be able to account for words appearing multiple times throughout an abstract, which typically reinfornces its importance. Using the frequency method compared to the presence/absence method will smooth out noise by amplifying strong signals and dampening weaker signals. \n",
    "\n",
    "In terms of preprocessing the text, I began with turning all the words to lowercase, so, for example, For and for will be allocated to the same frequency counter. I tokenize the raw text by utilising the re library's findall function, where I found sequences of word characters that are bounded between non-word characters. This is used to seperate the words in each paragraph, whilst filtering out spaces, punctuation, indentation, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2 - Model and Improvements Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my Standard NBC implementation, the classifier iterates through all of the training data, and for each instance, updates the appropriate project's word counter. Next, we go through each potential classification, and assign all words that appeared through the training data a probability it will occur, given that its respective class has occured. From here, the classifier has all the required parts to utilize Bayes Theorem to make classifications given test data. For each instance we want to classify, we find the probability for it being any of our 4 classes. The NBC then classifies the instance as the most likely project. \n",
    "\n",
    "In order to improve the original implementation, I implemented a variety of techniques, including noise fitration, hyperparameter tuning, and n-grams.\n",
    "\n",
    "- Firstly, I generated a stopword list using ChatGPT, consisting of noisy words such as 'with' or 'and'. I adjusted my preprocessing function to not acknowledge words in this list. This lead to a very slight improvement of roughly ~0.3%. I also attempted to filter out uncommon words, for example words that appeared less than 3 times throughout the entire training set, but this little to no effect. \n",
    "\n",
    "- Next, I adjusted the Laplace smoothing equation to use hyperparameter α, where the equation can be given in the form: (count + α)/(count+α|V|). After tuning α, I found that α=6 gave the best results. I also noticed that the training dataset is extremely unbalanced, with 'W': 2303, 'A': 1732, 'G': 232, 'S': 133. Therefore, instead of scaling the original project probabilities by size, I instead normalized them to add up to 1. Overall, this brought my score up to roughly ~97.5%. \n",
    "\n",
    "- I then implemented various forms of n-grams in hopes of capturing contextual dependancies. After testing the classifier with solely bigrams, solely trigrams, and various of variations, I found that the most accurate implementation was unigrams and bigrams (Trigrams led to worse classification). By adding bigrams however, this massively expanded my feature space, leading to a much sparser resulting dataset. To combat this, I had to retune my α hyperparamter value. This makes sense, as the feature dimensionality was increased, and a larger value such as 6 wouldn't pick up on useful signals. After re-tuning, I found that α=0.007 lead to the highest accuracy, at roughly ~97.8%. \n",
    "\n",
    "- Lastly, I used my print_top_features() function and my compare() function in order determine the most present/important words in identifying each class. With this information, I created a boosting dataset to give certain words more value for certain projects. For example, the bigram \"web_security\" has an additional weight of 0.9 towards the \"S\" classification. After some fine-tuning with these booster, I achieved my highest cross-validation score of 98.3%, and my highest Kaggle submission of 98.1%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3 - Evaluation Procedure Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to validate my data, I used a 5-fold cross validation. At the expense of requiring more computation and time, this method allowed for all the given data to be used to train and validate, and has lower variance, as the data will (very likely) be split more evenly as the split occurs 5 times, and the results are averaged. Specifically, I used sklearn library's StratifiedKFold method to randomly split my data into 5 folds. Then, on each fold, my model was trained and validated, and the overall accuracy was found by taking the average accuracy for each of the folds. (The accuracy was found by comparing each of my guesses to the correct answer in the given validation set, and finding correct/all)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4 - Standard / Improved Training/validation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "include and explain the training/validation results for the standard and improved Naive Bayes model. You can summarize results using tables (or plots), but all results have to be explained descriptively as well.\n",
    "\n",
    "From my cross-validation tests, we find: <br><br>\n",
    "\n",
    "5 Fold Cross-Validation Test on Standard Classifier:\n",
    "\n",
    "Fold 1 Accuracy: 93.75% |\n",
    "Fold 2 Accuracy: 94.20% |\n",
    "Fold 3 Accuracy: 94.66% |\n",
    "Fold 4 Accuracy: 94.77% |\n",
    "Fold 5 Accuracy: 96.25% |\n",
    "\n",
    "Average cross-validated accuracy: 94.73%\n",
    "<br><br><br>\n",
    "5 Fold Cross-Validation Test on Improved Classifier:\n",
    "\n",
    "Fold 1 Accuracy: 98.64% |\n",
    "Fold 2 Accuracy: 97.50% |\n",
    "Fold 3 Accuracy: 98.52% |\n",
    "Fold 4 Accuracy: 98.41% |\n",
    "Fold 5 Accuracy: 98.41% |\n",
    "\n",
    "Average cross-validated accuracy is 98.30%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers for Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Tests Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard CLASSIFIER\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "\n",
    "def standard_preprocess(text):\n",
    "    # Lowercase and keep only words\n",
    "    text = text.lower()\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    return words\n",
    "\n",
    "def StandardClassifier(X_train, X_val, y_train):\n",
    "    projs = [\"A\", \"S\", \"G\", \"W\"]\n",
    "    word_counts = defaultdict(Counter)\n",
    "    total_words = {\"A\":0,\"S\":0,\"G\":0,\"W\":0}\n",
    "    all_words = Counter()\n",
    "    num_projs = {\"A\":0,\"S\":0,\"G\":0,\"W\":0}\n",
    "    total_instances = len(X_train)\n",
    "\n",
    "    word_counts[\"A\"] = Counter()\n",
    "    word_counts[\"S\"] = Counter()\n",
    "    word_counts[\"G\"] = Counter()\n",
    "    word_counts[\"W\"] = Counter()\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        pred, desc = y_train[i], X_train[i]\n",
    "\n",
    "        pp_desc = standard_preprocess(desc)\n",
    "\n",
    "        word_counts[pred].update(pp_desc)\n",
    "        total_words[pred] += len(pp_desc)\n",
    "        all_words.update(pp_desc)\n",
    "        num_projs[pred] += 1\n",
    "\n",
    "    vocab_size = len(all_words)\n",
    "\n",
    "    class_weights = {proj: 1.0 / num_projs[proj] for proj in projs}\n",
    "    total_weight = sum(class_weights.values()) \n",
    "    proj_probs = []\n",
    "    for proj in projs:\n",
    "        weighted_prob = class_weights[proj] / total_weight  # Normalize so that sum is 1\n",
    "        proj_probs.append(weighted_prob)\n",
    "\n",
    "    word_probs = defaultdict(Counter)\n",
    "    word_probs[\"A\"] = Counter()\n",
    "    word_probs[\"S\"] = Counter()\n",
    "    word_probs[\"G\"] = Counter()\n",
    "    word_probs[\"W\"] = Counter()\n",
    "\n",
    "    #Calculating probabilities for each word/bigram\n",
    "    for proj in projs:\n",
    "        focus_dict = word_counts[proj]\n",
    "        denom = total_words[proj] + total_instances\n",
    "        \n",
    "        #Calculating likelihoods with Laplace smoothing\n",
    "        for word in all_words:\n",
    "            if word not in focus_dict:\n",
    "                word_probs[proj][word] = 1/denom\n",
    "            else:\n",
    "                word_probs[proj][word] = (focus_dict[word]+1)/denom\n",
    "        \n",
    "    #Initialzing classification variables\n",
    "    temp = np.log(proj_probs)\n",
    "    class_probs = {}\n",
    "    for i in range(len(temp)):\n",
    "        class_probs[projs[i]] = temp[i]\n",
    "    classifications = []\n",
    "\n",
    "    #Classify test data\n",
    "    for desc in X_val:\n",
    "        pp_desc = standard_preprocess(desc)\n",
    "\n",
    "        cur_class_probs = class_probs.copy()\n",
    "        for proj in projs:\n",
    "            #Unigram Probs\n",
    "            for word in pp_desc:\n",
    "                cur_class_probs[proj] += np.log(word_probs[proj].get(word, 1 / denom))\n",
    "\n",
    "        #find project with highest probability, and make that the prediction\n",
    "        best_proj = max(cur_class_probs, key=cur_class_probs.get)\n",
    "        classifications.append(best_proj)\n",
    "    \n",
    "    return classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Improved CLASSIFIER\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "\n",
    "stop_words = {\n",
    "    'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\",\n",
    "    'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by',\n",
    "    'can', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing',\n",
    "    \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\",\n",
    "    'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself',\n",
    "    'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is',\n",
    "    \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'no',\n",
    "    'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves',\n",
    "    'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so',\n",
    "    'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then',\n",
    "    'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those',\n",
    "    'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\",\n",
    "    \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while',\n",
    "    'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\",\n",
    "    \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', 'project', 'the_project', ' '\n",
    "}\n",
    "\n",
    "log_boosts = {\n",
    "    #Artificial Intelligence\n",
    "    \"A\": {\n",
    "        \"system\": 1.2,\n",
    "        \"learning\": 1.2,\n",
    "        \"making\": 0.7,\n",
    "        \"data\": 0.7,\n",
    "        \"decision\": 1,\n",
    "        \"agents\": 1,\n",
    "        \"real\": 0.5,\n",
    "        \"machine\": 1,\n",
    "        \"time\": 0.4,\n",
    "        \"using\": 0.3,\n",
    "        \"control\": 0.5,\n",
    "        \"autonomous\": 0.6,\n",
    "        \"machine_learning\": 1\n",
    "\n",
    "    },\n",
    "    #Security\n",
    "    \"S\": {\n",
    "        \"security\": 1.2,\n",
    "        \"data\": 0.8,\n",
    "        \"system\": 0.7,\n",
    "        \"authentication\": 1.2,\n",
    "        \"web\": 0.5,\n",
    "        \"user\": 0.7,\n",
    "        \"network\": 2,\n",
    "        \"learning\": 0.4,\n",
    "        \"secure\": 1.5,\n",
    "        \"based\": 0.3,\n",
    "        \"web_security\": 0.9,\n",
    "    },\n",
    "    #Game\n",
    "    \"G\": {\n",
    "        \"game\": 2,\n",
    "        \"platform\": 0.8,\n",
    "        \"multiplayer\": 1.5,\n",
    "        \"real\": 0.6,\n",
    "        \"experience\": 1,\n",
    "        \"time\": 0.5,\n",
    "        \"players\": 0.5,\n",
    "        \"user\": 0.6,\n",
    "        \"time\": 0.5\n",
    "    },\n",
    "    # Web Development\n",
    "    \"W\": {\n",
    "        \"system\" : 0.4,\n",
    "        \"user\" : 1.2, \n",
    "        \"website\": 1,\n",
    "        \"frontend\": 0.6,\n",
    "        \"backend\": 0.6,\n",
    "        \"interface\": 0.5,\n",
    "        \"react\": 0.5,\n",
    "        \"html\": 0.5,\n",
    "        \"css\": 0.5,\n",
    "        \"api\": 0.6,\n",
    "        \"dynamic\": 0.5,\n",
    "        \"framework\": 0.5,\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def print_top_features(word_probs, top_n=10):\n",
    "    for proj in word_probs:\n",
    "        print(f\"\\nTop {top_n} features for class '{proj}':\")\n",
    "        most_common = sorted(word_probs[proj].items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        for word, prob in most_common:\n",
    "            print(f\"  {word}: {prob:.6f}\")\n",
    "\n",
    "def improved_preprocess(text):\n",
    "    text = text.lower()\n",
    "    words = [w for w in re.findall(r'\\b\\w+\\b', text) if w not in stop_words]\n",
    "    bigrams = []\n",
    "    bigrams = [f\"{words[i-1]}_{words[i]}\" for i in range(1, len(words))]\n",
    "    return words + bigrams\n",
    "\n",
    "\n",
    "def ImprovedClassifier(alpha, X_train, X_val, y_train):\n",
    "    projs = [\"A\", \"S\", \"G\", \"W\"]\n",
    "    word_counts = defaultdict(Counter)\n",
    "    total_words = {\"A\":0,\"S\":0,\"G\":0,\"W\":0}\n",
    "    all_words = Counter()\n",
    "    num_projs = {\"A\":0,\"S\":0,\"G\":0,\"W\":0}\n",
    "    total_instances = len(X_train)\n",
    "\n",
    "    word_counts[\"A\"] = Counter()\n",
    "    word_counts[\"S\"] = Counter()\n",
    "    word_counts[\"G\"] = Counter()\n",
    "    word_counts[\"W\"] = Counter()\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        pred, desc = y_train[i], X_train[i]\n",
    "\n",
    "        pp_desc = improved_preprocess(desc)\n",
    "\n",
    "        word_counts[pred].update(pp_desc)\n",
    "        total_words[pred] += len(pp_desc)\n",
    "        all_words.update(pp_desc)\n",
    "        num_projs[pred] += 1\n",
    "\n",
    "    temp = Counter()\n",
    "    for word, count in all_words.items():\n",
    "        if \"_\" in word:\n",
    "            if count >= 3:\n",
    "                temp[word] = count\n",
    "        else:\n",
    "            temp[word] = count\n",
    "    all_words = temp\n",
    "\n",
    "    vocab_size = len(all_words)\n",
    "\n",
    "    class_weights = {proj: num_projs[proj] / total_instances for proj in projs}\n",
    "    total_weight = sum(class_weights.values()) \n",
    "    proj_probs = []\n",
    "    for proj in projs:\n",
    "        weighted_prob = class_weights[proj] / total_weight  # Normalize so that sum is 1\n",
    "        proj_probs.append(0)\n",
    "\n",
    "    word_probs = defaultdict(Counter)\n",
    "    word_probs[\"A\"] = Counter()\n",
    "    word_probs[\"S\"] = Counter()\n",
    "    word_probs[\"G\"] = Counter()\n",
    "    word_probs[\"W\"] = Counter()\n",
    "\n",
    "    #Calculating probabilities for each word/bigram\n",
    "    for proj in projs:\n",
    "        focus_dict = word_counts[proj]\n",
    "        denom = total_words[proj] + alpha*vocab_size\n",
    "        \n",
    "        #Calculating likelihoods with Laplace smoothing\n",
    "        for word in all_words:\n",
    "            if word not in focus_dict:\n",
    "                word_probs[proj][word] = alpha/denom\n",
    "            else:\n",
    "                word_probs[proj][word] = (focus_dict[word]+alpha)/denom\n",
    "\n",
    "    #Initialzing classification variables\n",
    "    # temp = np.log(proj_probs)\n",
    "    class_probs = {}\n",
    "    for i in range(len(projs)):\n",
    "        class_probs[projs[i]] = 0\n",
    "    classifications = []\n",
    "\n",
    "    counter = 0\n",
    "    #Classify test data\n",
    "    for desc in X_val:\n",
    "        counter += 1\n",
    "        pp_desc = improved_preprocess(desc)\n",
    "\n",
    "        cur_class_probs = class_probs.copy()\n",
    "        for proj in projs:\n",
    "            for word in pp_desc:\n",
    "                prob = word_probs[proj].get(word, alpha / total_words[proj] + alpha*vocab_size)\n",
    "                log_boost = log_boosts.get(proj, {}).get(word, 0.0)\n",
    "                cur_class_probs[proj] += np.log(prob) + log_boost\n",
    "\n",
    "        # if counter == 36:\n",
    "        #     print(cur_class_probs)\n",
    "        #find project with highest probability, and make that the prediction\n",
    "        best_proj = max(cur_class_probs, key=cur_class_probs.get)\n",
    "        classifications.append(best_proj)\n",
    "\n",
    "    # print_top_features(word_probs, 10)\n",
    "\n",
    "    return classifications\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Kaggle Submissions\n",
    "def Submission(name):\n",
    "    train = pd.read_csv(\"train.csv\")\n",
    "    test = pd.read_csv(\"test.csv\")\n",
    "    X_train = train[\"Description\"].values\n",
    "    y_train = train[\"Class\"].values\n",
    "    X_val = test[\"Description\"].values\n",
    "\n",
    "    classifications = ImprovedClassifier(3, X_train, X_val, y_train)\n",
    "\n",
    "    #Format data into CSV file\n",
    "    csv_cols = []\n",
    "    for i,c in enumerate(classifications):\n",
    "        csv_cols.append((i+1, c))\n",
    "    output_df = pd.DataFrame(csv_cols, columns=[\"Id\", \"Class\"])\n",
    "    output_df.to_csv(f\"{name}\", index=False)\n",
    "\n",
    "Submission(\"prediction28.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 Fold Cross-Validation Method\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def CheckAccuracy(classifications, y_val):\n",
    "    correct = 0\n",
    "    for i in range(len(classifications)):\n",
    "        if classifications[i] == y_val[i]:\n",
    "            correct += 1\n",
    "    return correct/len(classifications)\n",
    "    \n",
    "    \n",
    "def CrossValidate(alpha):\n",
    "    # print(f\"5 Fold Cross-Validation Test on Classifier(s):\\n\")\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "    X = df[\"Description\"].values\n",
    "    y = df[\"Class\"].values\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "        # Split the data\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        # classifications = StandardClassifier(X_train, X_val, y_train)\n",
    "        # accuracy = CheckAccuracy(classifications, y_val)\n",
    "        # accuracies.append(accuracy)\n",
    "        # print(f\"Fold {fold+1} Accuracy for Standard Classifier: {accuracy:.2%}.\")\n",
    "\n",
    "        classifications = ImprovedClassifier(alpha, X_train, X_val, y_train)\n",
    "        accuracy = CheckAccuracy(classifications, y_val)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"Fold {fold+1} Accuracy for Improved Classifier: {accuracy:.2%}.\")\n",
    "\n",
    "    print(f\"\\n(a = {alpha}) Average cross-validated accuracy is {np.mean(accuracies):.2%}\\n\")\n",
    "\n",
    "\n",
    "for i in [0.006]:\n",
    "    CrossValidate(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88) testing.csv: S, other: W\n",
      "our team implemented an autonomous vehicle control system using machine learning algorithms we focused on developing a system that can navigate a vehicle without human intervention using ai based techniques to achieve this we utilized ros robot operating system for the vehicles control and simulation we trained machine learning models such as deep neural networks using tensorflow and keras for tasks like object detection lane detection and decision making we also used opencv for image processing tasks the input to the system was real time sensor data including data from cameras lidar and radar sensors the machine learning algorithms processed this data to detect obstacles identify lane markings and make decisions for steering acceleration and braking the output of the project was a fully functional autonomous vehicle control system that could effectively navigate through different environments including urban and highway scenarios the system could interpret its surroundings make driving decisions in real time and execute them through its control interface the project demonstrated the integration of machine learning algorithms with autonomous vehicle control showcasing the potential for ai to enable safe and efficient autonomous driving the use of advanced technologies and libraries facilitated the development of a robust and reliable system capable of responding to dynamic and complex environments\n",
      "Counter({'system': 7, 'control': 5, 'autonomous': 4, 'vehicle': 4, 'machine': 4, 'learning': 4, 'machine_learning': 4, 'using': 3, 'algorithms': 3, 'data': 3, 'autonomous_vehicle': 3, 'vehicle_control': 3, 'learning_algorithms': 3, 'navigate': 2, 'ai': 2, 'tasks': 2, 'detection': 2, 'lane': 2, 'real': 2, 'time': 2, 'including': 2, 'make': 2, 'decisions': 2, 'environments': 2, 'driving': 2, 'control_system': 2, 'real_time': 2, 'team': 1, 'implemented': 1, 'focused': 1, 'developing': 1, 'without': 1, 'human': 1, 'intervention': 1, 'based': 1, 'techniques': 1, 'achieve': 1, 'utilized': 1, 'ros': 1, 'robot': 1, 'operating': 1, 'vehicles': 1, 'simulation': 1, 'trained': 1, 'models': 1, 'deep': 1, 'neural': 1, 'networks': 1, 'tensorflow': 1, 'keras': 1, 'like': 1, 'object': 1, 'decision': 1, 'making': 1, 'also': 1, 'used': 1, 'opencv': 1, 'image': 1, 'processing': 1, 'input': 1, 'sensor': 1, 'cameras': 1, 'lidar': 1, 'radar': 1, 'sensors': 1, 'processed': 1, 'detect': 1, 'obstacles': 1, 'identify': 1, 'markings': 1, 'steering': 1, 'acceleration': 1, 'braking': 1, 'output': 1, 'fully': 1, 'functional': 1, 'effectively': 1, 'different': 1, 'urban': 1, 'highway': 1, 'scenarios': 1, 'interpret': 1, 'surroundings': 1, 'execute': 1, 'interface': 1, 'demonstrated': 1, 'integration': 1, 'showcasing': 1, 'potential': 1, 'enable': 1, 'safe': 1, 'efficient': 1, 'use': 1, 'advanced': 1, 'technologies': 1, 'libraries': 1, 'facilitated': 1, 'development': 1, 'robust': 1, 'reliable': 1, 'capable': 1, 'responding': 1, 'dynamic': 1, 'complex': 1, 'team_implemented': 1, 'implemented_autonomous': 1, 'system_using': 1, 'using_machine': 1, 'algorithms_focused': 1, 'focused_developing': 1, 'developing_system': 1, 'system_navigate': 1, 'navigate_vehicle': 1, 'vehicle_without': 1, 'without_human': 1, 'human_intervention': 1, 'intervention_using': 1, 'using_ai': 1, 'ai_based': 1, 'based_techniques': 1, 'techniques_achieve': 1, 'achieve_utilized': 1, 'utilized_ros': 1, 'ros_robot': 1, 'robot_operating': 1, 'operating_system': 1, 'system_vehicles': 1, 'vehicles_control': 1, 'control_simulation': 1, 'simulation_trained': 1, 'trained_machine': 1, 'learning_models': 1, 'models_deep': 1, 'deep_neural': 1, 'neural_networks': 1, 'networks_using': 1, 'using_tensorflow': 1, 'tensorflow_keras': 1, 'keras_tasks': 1, 'tasks_like': 1, 'like_object': 1, 'object_detection': 1, 'detection_lane': 1, 'lane_detection': 1, 'detection_decision': 1, 'decision_making': 1, 'making_also': 1, 'also_used': 1, 'used_opencv': 1, 'opencv_image': 1, 'image_processing': 1, 'processing_tasks': 1, 'tasks_input': 1, 'input_system': 1, 'system_real': 1, 'time_sensor': 1, 'sensor_data': 1, 'data_including': 1, 'including_data': 1, 'data_cameras': 1, 'cameras_lidar': 1, 'lidar_radar': 1, 'radar_sensors': 1, 'sensors_machine': 1, 'algorithms_processed': 1, 'processed_data': 1, 'data_detect': 1, 'detect_obstacles': 1, 'obstacles_identify': 1, 'identify_lane': 1, 'lane_markings': 1, 'markings_make': 1, 'make_decisions': 1, 'decisions_steering': 1, 'steering_acceleration': 1, 'acceleration_braking': 1, 'braking_output': 1, 'output_fully': 1, 'fully_functional': 1, 'functional_autonomous': 1, 'system_effectively': 1, 'effectively_navigate': 1, 'navigate_different': 1, 'different_environments': 1, 'environments_including': 1, 'including_urban': 1, 'urban_highway': 1, 'highway_scenarios': 1, 'scenarios_system': 1, 'system_interpret': 1, 'interpret_surroundings': 1, 'surroundings_make': 1, 'make_driving': 1, 'driving_decisions': 1, 'decisions_real': 1, 'time_execute': 1, 'execute_control': 1, 'control_interface': 1, 'interface_demonstrated': 1, 'demonstrated_integration': 1, 'integration_machine': 1, 'algorithms_autonomous': 1, 'control_showcasing': 1, 'showcasing_potential': 1, 'potential_ai': 1, 'ai_enable': 1, 'enable_safe': 1, 'safe_efficient': 1, 'efficient_autonomous': 1, 'autonomous_driving': 1, 'driving_use': 1, 'use_advanced': 1, 'advanced_technologies': 1, 'technologies_libraries': 1, 'libraries_facilitated': 1, 'facilitated_development': 1, 'development_robust': 1, 'robust_reliable': 1, 'reliable_system': 1, 'system_capable': 1, 'capable_responding': 1, 'responding_dynamic': 1, 'dynamic_complex': 1, 'complex_environments': 1})\n",
      "(187) testing.csv: W, other: S\n",
      "the project focuses on enhancing web security through vulnerability assessment and penetration testing we implemented the project by utilizing various tools and technologies such as kali linux burp suite owasp zap and nmap for vulnerability scanning and penetration testing in addition we used popular web security libraries such as open web application security project owasp and the metasploit framework the primary goal of the project was to develop a comprehensive vulnerability assessment and penetration testing tool that can effectively identify and mitigate web application vulnerabilities the tool includes features such as automated scanning for common web application vulnerabilities like sql injection cross site scripting xss and csrf cross site request forgery as well as manual testing capabilities for more advanced vulnerabilities furthermore the tool provides detailed reports and recommendations for fixing identified vulnerabilities helping web developers and security professionals to understand and address potential security issues in web applications it also includes features for simulating real world attack scenarios to assess the overall security posture of web applications by leveraging these technologies and tools the project aims to empower organizations to proactively identify and remediate security weaknesses in their web applications ultimately enhancing their overall web security posture the output of the project is a user friendly and powerful web security assessment and penetration testing tool that can be utilized by security experts and developers to secure web applications against potential threats and cyber attacks\n",
      "Counter({'web': 12, 'security': 10, 'testing': 5, 'penetration': 4, 'tool': 4, 'vulnerabilities': 4, 'applications': 4, 'web_security': 4, 'penetration_testing': 4, 'web_applications': 4, 'vulnerability': 3, 'assessment': 3, 'application': 3, 'assessment_penetration': 3, 'web_application': 3, 'enhancing': 2, 'tools': 2, 'technologies': 2, 'owasp': 2, 'scanning': 2, 'identify': 2, 'includes': 2, 'features': 2, 'cross': 2, 'site': 2, 'developers': 2, 'potential': 2, 'overall': 2, 'posture': 2, 'vulnerability_assessment': 2, 'testing_tool': 2, 'application_vulnerabilities': 2, 'includes_features': 2, 'cross_site': 2, 'security_posture': 2, 'focuses': 1, 'implemented': 1, 'utilizing': 1, 'various': 1, 'kali': 1, 'linux': 1, 'burp': 1, 'suite': 1, 'zap': 1, 'nmap': 1, 'addition': 1, 'used': 1, 'popular': 1, 'libraries': 1, 'open': 1, 'metasploit': 1, 'framework': 1, 'primary': 1, 'goal': 1, 'develop': 1, 'comprehensive': 1, 'effectively': 1, 'mitigate': 1, 'automated': 1, 'common': 1, 'like': 1, 'sql': 1, 'injection': 1, 'scripting': 1, 'xss': 1, 'csrf': 1, 'request': 1, 'forgery': 1, 'well': 1, 'manual': 1, 'capabilities': 1, 'advanced': 1, 'furthermore': 1, 'provides': 1, 'detailed': 1, 'reports': 1, 'recommendations': 1, 'fixing': 1, 'identified': 1, 'helping': 1, 'professionals': 1, 'understand': 1, 'address': 1, 'issues': 1, 'also': 1, 'simulating': 1, 'real': 1, 'world': 1, 'attack': 1, 'scenarios': 1, 'assess': 1, 'leveraging': 1, 'aims': 1, 'empower': 1, 'organizations': 1, 'proactively': 1, 'remediate': 1, 'weaknesses': 1, 'ultimately': 1, 'output': 1, 'user': 1, 'friendly': 1, 'powerful': 1, 'utilized': 1, 'experts': 1, 'secure': 1, 'threats': 1, 'cyber': 1, 'attacks': 1, 'focuses_enhancing': 1, 'enhancing_web': 1, 'security_vulnerability': 1, 'testing_implemented': 1, 'implemented_utilizing': 1, 'utilizing_various': 1, 'various_tools': 1, 'tools_technologies': 1, 'technologies_kali': 1, 'kali_linux': 1, 'linux_burp': 1, 'burp_suite': 1, 'suite_owasp': 1, 'owasp_zap': 1, 'zap_nmap': 1, 'nmap_vulnerability': 1, 'vulnerability_scanning': 1, 'scanning_penetration': 1, 'testing_addition': 1, 'addition_used': 1, 'used_popular': 1, 'popular_web': 1, 'security_libraries': 1, 'libraries_open': 1, 'open_web': 1, 'application_security': 1, 'security_owasp': 1, 'owasp_metasploit': 1, 'metasploit_framework': 1, 'framework_primary': 1, 'primary_goal': 1, 'goal_develop': 1, 'develop_comprehensive': 1, 'comprehensive_vulnerability': 1, 'tool_effectively': 1, 'effectively_identify': 1, 'identify_mitigate': 1, 'mitigate_web': 1, 'vulnerabilities_tool': 1, 'tool_includes': 1, 'features_automated': 1, 'automated_scanning': 1, 'scanning_common': 1, 'common_web': 1, 'vulnerabilities_like': 1, 'like_sql': 1, 'sql_injection': 1, 'injection_cross': 1, 'site_scripting': 1, 'scripting_xss': 1, 'xss_csrf': 1, 'csrf_cross': 1, 'site_request': 1, 'request_forgery': 1, 'forgery_well': 1, 'well_manual': 1, 'manual_testing': 1, 'testing_capabilities': 1, 'capabilities_advanced': 1, 'advanced_vulnerabilities': 1, 'vulnerabilities_furthermore': 1, 'furthermore_tool': 1, 'tool_provides': 1, 'provides_detailed': 1, 'detailed_reports': 1, 'reports_recommendations': 1, 'recommendations_fixing': 1, 'fixing_identified': 1, 'identified_vulnerabilities': 1, 'vulnerabilities_helping': 1, 'helping_web': 1, 'web_developers': 1, 'developers_security': 1, 'security_professionals': 1, 'professionals_understand': 1, 'understand_address': 1, 'address_potential': 1, 'potential_security': 1, 'security_issues': 1, 'issues_web': 1, 'applications_also': 1, 'also_includes': 1, 'features_simulating': 1, 'simulating_real': 1, 'real_world': 1, 'world_attack': 1, 'attack_scenarios': 1, 'scenarios_assess': 1, 'assess_overall': 1, 'overall_security': 1, 'posture_web': 1, 'applications_leveraging': 1, 'leveraging_technologies': 1, 'technologies_tools': 1, 'tools_aims': 1, 'aims_empower': 1, 'empower_organizations': 1, 'organizations_proactively': 1, 'proactively_identify': 1, 'identify_remediate': 1, 'remediate_security': 1, 'security_weaknesses': 1, 'weaknesses_web': 1, 'applications_ultimately': 1, 'ultimately_enhancing': 1, 'enhancing_overall': 1, 'overall_web': 1, 'posture_output': 1, 'output_user': 1, 'user_friendly': 1, 'friendly_powerful': 1, 'powerful_web': 1, 'security_assessment': 1, 'tool_utilized': 1, 'utilized_security': 1, 'security_experts': 1, 'experts_developers': 1, 'developers_secure': 1, 'secure_web': 1, 'applications_potential': 1, 'potential_threats': 1, 'threats_cyber': 1, 'cyber_attacks': 1})\n",
      "(451) testing.csv: W, other: S\n",
      "our team developed a digital asset management system for small businesses focusing on providing an intuitive and efficient platform for organizing and accessing digital assets we used html css and javascript for the frontend creating a user friendly interface for uploading categorizing and searching for digital assets for the backend we utilized nodejs with express for server side logic and api development while storing the assets in a mongodb database using mongoose for seamless data management the system allows users to securely upload various types of digital assets such as images documents and videos and tag them with relevant metadata for easy retrieval we integrated authentication and authorization features using json web tokens jwt to ensure data privacy and access control additionally we implemented a robust search functionality leveraging elasticsearch to enable users to quickly locate specific assets based on keywords and metadata to enhance user experience we incorporated responsive design principles making the system accessible across different devices and screen sizes we also integrated cloud storage providers like amazon s3 or google cloud storage to offer scalable and reliable storage solutions for the uploaded assets overall the digital asset management system aimed to streamline the process of organizing and managing digital resources for small businesses providing a centralized and accessible repository for their valuable assets the projects focus was on delivering a user friendly interface robust backend functionality and secure data management addressing the specific needs of small businesses in efficiently managing their digital assets\n",
      "Counter({'assets': 8, 'digital': 7, 'management': 4, 'system': 4, 'digital_assets': 4, 'small': 3, 'businesses': 3, 'user': 3, 'data': 3, 'storage': 3, 'management_system': 3, 'small_businesses': 3, 'asset': 2, 'providing': 2, 'organizing': 2, 'friendly': 2, 'interface': 2, 'backend': 2, 'using': 2, 'users': 2, 'metadata': 2, 'integrated': 2, 'robust': 2, 'functionality': 2, 'specific': 2, 'accessible': 2, 'cloud': 2, 'managing': 2, 'digital_asset': 2, 'asset_management': 2, 'user_friendly': 2, 'friendly_interface': 2, 'data_management': 2, 'cloud_storage': 2, 'managing_digital': 2, 'team': 1, 'developed': 1, 'focusing': 1, 'intuitive': 1, 'efficient': 1, 'platform': 1, 'accessing': 1, 'used': 1, 'html': 1, 'css': 1, 'javascript': 1, 'frontend': 1, 'creating': 1, 'uploading': 1, 'categorizing': 1, 'searching': 1, 'utilized': 1, 'nodejs': 1, 'express': 1, 'server': 1, 'side': 1, 'logic': 1, 'api': 1, 'development': 1, 'storing': 1, 'mongodb': 1, 'database': 1, 'mongoose': 1, 'seamless': 1, 'allows': 1, 'securely': 1, 'upload': 1, 'various': 1, 'types': 1, 'images': 1, 'documents': 1, 'videos': 1, 'tag': 1, 'relevant': 1, 'easy': 1, 'retrieval': 1, 'authentication': 1, 'authorization': 1, 'features': 1, 'json': 1, 'web': 1, 'tokens': 1, 'jwt': 1, 'ensure': 1, 'privacy': 1, 'access': 1, 'control': 1, 'additionally': 1, 'implemented': 1, 'search': 1, 'leveraging': 1, 'elasticsearch': 1, 'enable': 1, 'quickly': 1, 'locate': 1, 'based': 1, 'keywords': 1, 'enhance': 1, 'experience': 1, 'incorporated': 1, 'responsive': 1, 'design': 1, 'principles': 1, 'making': 1, 'across': 1, 'different': 1, 'devices': 1, 'screen': 1, 'sizes': 1, 'also': 1, 'providers': 1, 'like': 1, 'amazon': 1, 's3': 1, 'google': 1, 'offer': 1, 'scalable': 1, 'reliable': 1, 'solutions': 1, 'uploaded': 1, 'overall': 1, 'aimed': 1, 'streamline': 1, 'process': 1, 'resources': 1, 'centralized': 1, 'repository': 1, 'valuable': 1, 'projects': 1, 'focus': 1, 'delivering': 1, 'secure': 1, 'addressing': 1, 'needs': 1, 'efficiently': 1, 'team_developed': 1, 'developed_digital': 1, 'system_small': 1, 'businesses_focusing': 1, 'focusing_providing': 1, 'providing_intuitive': 1, 'intuitive_efficient': 1, 'efficient_platform': 1, 'platform_organizing': 1, 'organizing_accessing': 1, 'accessing_digital': 1, 'assets_used': 1, 'used_html': 1, 'html_css': 1, 'css_javascript': 1, 'javascript_frontend': 1, 'frontend_creating': 1, 'creating_user': 1, 'interface_uploading': 1, 'uploading_categorizing': 1, 'categorizing_searching': 1, 'searching_digital': 1, 'assets_backend': 1, 'backend_utilized': 1, 'utilized_nodejs': 1, 'nodejs_express': 1, 'express_server': 1, 'server_side': 1, 'side_logic': 1, 'logic_api': 1, 'api_development': 1, 'development_storing': 1, 'storing_assets': 1, 'assets_mongodb': 1, 'mongodb_database': 1, 'database_using': 1, 'using_mongoose': 1, 'mongoose_seamless': 1, 'seamless_data': 1, 'system_allows': 1, 'allows_users': 1, 'users_securely': 1, 'securely_upload': 1, 'upload_various': 1, 'various_types': 1, 'types_digital': 1, 'assets_images': 1, 'images_documents': 1, 'documents_videos': 1, 'videos_tag': 1, 'tag_relevant': 1, 'relevant_metadata': 1, 'metadata_easy': 1, 'easy_retrieval': 1, 'retrieval_integrated': 1, 'integrated_authentication': 1, 'authentication_authorization': 1, 'authorization_features': 1, 'features_using': 1, 'using_json': 1, 'json_web': 1, 'web_tokens': 1, 'tokens_jwt': 1, 'jwt_ensure': 1, 'ensure_data': 1, 'data_privacy': 1, 'privacy_access': 1, 'access_control': 1, 'control_additionally': 1, 'additionally_implemented': 1, 'implemented_robust': 1, 'robust_search': 1, 'search_functionality': 1, 'functionality_leveraging': 1, 'leveraging_elasticsearch': 1, 'elasticsearch_enable': 1, 'enable_users': 1, 'users_quickly': 1, 'quickly_locate': 1, 'locate_specific': 1, 'specific_assets': 1, 'assets_based': 1, 'based_keywords': 1, 'keywords_metadata': 1, 'metadata_enhance': 1, 'enhance_user': 1, 'user_experience': 1, 'experience_incorporated': 1, 'incorporated_responsive': 1, 'responsive_design': 1, 'design_principles': 1, 'principles_making': 1, 'making_system': 1, 'system_accessible': 1, 'accessible_across': 1, 'across_different': 1, 'different_devices': 1, 'devices_screen': 1, 'screen_sizes': 1, 'sizes_also': 1, 'also_integrated': 1, 'integrated_cloud': 1, 'storage_providers': 1, 'providers_like': 1, 'like_amazon': 1, 'amazon_s3': 1, 's3_google': 1, 'google_cloud': 1, 'storage_offer': 1, 'offer_scalable': 1, 'scalable_reliable': 1, 'reliable_storage': 1, 'storage_solutions': 1, 'solutions_uploaded': 1, 'uploaded_assets': 1, 'assets_overall': 1, 'overall_digital': 1, 'system_aimed': 1, 'aimed_streamline': 1, 'streamline_process': 1, 'process_organizing': 1, 'organizing_managing': 1, 'digital_resources': 1, 'resources_small': 1, 'businesses_providing': 1, 'providing_centralized': 1, 'centralized_accessible': 1, 'accessible_repository': 1, 'repository_valuable': 1, 'valuable_assets': 1, 'assets_projects': 1, 'projects_focus': 1, 'focus_delivering': 1, 'delivering_user': 1, 'interface_robust': 1, 'robust_backend': 1, 'backend_functionality': 1, 'functionality_secure': 1, 'secure_data': 1, 'management_addressing': 1, 'addressing_specific': 1, 'specific_needs': 1, 'needs_small': 1, 'businesses_efficiently': 1, 'efficiently_managing': 1})\n",
      "(839) testing.csv: S, other: A\n",
      "in the capstone project we designed and implemented rational agent systems in an open source environment to achieve this we utilized java as our primary programming language along with the open source tool jade java agent development framework for building multi agent systems jade provided us with tools for implementing intelligent agents allowing us to create a simulation environment where these agents could interact and make decisions based on their rationality the project focused on creating a system where rational agents could communicate negotiate and make decisions autonomously simulating a real world environment we used various technologies and libraries such as jade java and potentially other open source tools for developing the multi agent system our aim was to demonstrate how rational agents can function within an open source environment showcasing their ability to collaborate and make logical choices in dynamic scenarios our implementation emphasized the importance of rational decision making and how it can be applied to real world problems using intelligent agent systems we designed a platform where these rational agents could interact exchange information negotiate and ultimately reach decisions that are in line with their objectives the project output was essentially a functioning system that showcased the capabilities of rational agent systems in an open source environment highlighting their potential applications in various domains such as smart cities supply chain management and autonomous systems our goal was to provide a tangible example of how rational agent systems can be leveraged to tackle complex problems in a distributed and open source setting\n",
      "Counter({'rational': 7, 'agent': 7, 'systems': 6, 'open': 6, 'source': 6, 'open_source': 6, 'environment': 5, 'agents': 5, 'agent_systems': 5, 'java': 3, 'jade': 3, 'make': 3, 'decisions': 3, 'system': 3, 'rational_agent': 3, 'source_environment': 3, 'rational_agents': 3, 'designed': 2, 'multi': 2, 'us': 2, 'tools': 2, 'intelligent': 2, 'interact': 2, 'negotiate': 2, 'real': 2, 'world': 2, 'various': 2, 'problems': 2, 'systems_open': 2, 'jade_java': 2, 'multi_agent': 2, 'agents_interact': 2, 'make_decisions': 2, 'real_world': 2, 'capstone': 1, 'implemented': 1, 'achieve': 1, 'utilized': 1, 'primary': 1, 'programming': 1, 'language': 1, 'along': 1, 'tool': 1, 'development': 1, 'framework': 1, 'building': 1, 'provided': 1, 'implementing': 1, 'allowing': 1, 'create': 1, 'simulation': 1, 'based': 1, 'rationality': 1, 'focused': 1, 'creating': 1, 'communicate': 1, 'autonomously': 1, 'simulating': 1, 'used': 1, 'technologies': 1, 'libraries': 1, 'potentially': 1, 'developing': 1, 'aim': 1, 'demonstrate': 1, 'function': 1, 'within': 1, 'showcasing': 1, 'ability': 1, 'collaborate': 1, 'logical': 1, 'choices': 1, 'dynamic': 1, 'scenarios': 1, 'implementation': 1, 'emphasized': 1, 'importance': 1, 'decision': 1, 'making': 1, 'applied': 1, 'using': 1, 'platform': 1, 'exchange': 1, 'information': 1, 'ultimately': 1, 'reach': 1, 'line': 1, 'objectives': 1, 'output': 1, 'essentially': 1, 'functioning': 1, 'showcased': 1, 'capabilities': 1, 'highlighting': 1, 'potential': 1, 'applications': 1, 'domains': 1, 'smart': 1, 'cities': 1, 'supply': 1, 'chain': 1, 'management': 1, 'autonomous': 1, 'goal': 1, 'provide': 1, 'tangible': 1, 'example': 1, 'leveraged': 1, 'tackle': 1, 'complex': 1, 'distributed': 1, 'setting': 1, 'capstone_designed': 1, 'designed_implemented': 1, 'implemented_rational': 1, 'environment_achieve': 1, 'achieve_utilized': 1, 'utilized_java': 1, 'java_primary': 1, 'primary_programming': 1, 'programming_language': 1, 'language_along': 1, 'along_open': 1, 'source_tool': 1, 'tool_jade': 1, 'java_agent': 1, 'agent_development': 1, 'development_framework': 1, 'framework_building': 1, 'building_multi': 1, 'systems_jade': 1, 'jade_provided': 1, 'provided_us': 1, 'us_tools': 1, 'tools_implementing': 1, 'implementing_intelligent': 1, 'intelligent_agents': 1, 'agents_allowing': 1, 'allowing_us': 1, 'us_create': 1, 'create_simulation': 1, 'simulation_environment': 1, 'environment_agents': 1, 'interact_make': 1, 'decisions_based': 1, 'based_rationality': 1, 'rationality_focused': 1, 'focused_creating': 1, 'creating_system': 1, 'system_rational': 1, 'agents_communicate': 1, 'communicate_negotiate': 1, 'negotiate_make': 1, 'decisions_autonomously': 1, 'autonomously_simulating': 1, 'simulating_real': 1, 'world_environment': 1, 'environment_used': 1, 'used_various': 1, 'various_technologies': 1, 'technologies_libraries': 1, 'libraries_jade': 1, 'java_potentially': 1, 'potentially_open': 1, 'source_tools': 1, 'tools_developing': 1, 'developing_multi': 1, 'agent_system': 1, 'system_aim': 1, 'aim_demonstrate': 1, 'demonstrate_rational': 1, 'agents_function': 1, 'function_within': 1, 'within_open': 1, 'environment_showcasing': 1, 'showcasing_ability': 1, 'ability_collaborate': 1, 'collaborate_make': 1, 'make_logical': 1, 'logical_choices': 1, 'choices_dynamic': 1, 'dynamic_scenarios': 1, 'scenarios_implementation': 1, 'implementation_emphasized': 1, 'emphasized_importance': 1, 'importance_rational': 1, 'rational_decision': 1, 'decision_making': 1, 'making_applied': 1, 'applied_real': 1, 'world_problems': 1, 'problems_using': 1, 'using_intelligent': 1, 'intelligent_agent': 1, 'systems_designed': 1, 'designed_platform': 1, 'platform_rational': 1, 'interact_exchange': 1, 'exchange_information': 1, 'information_negotiate': 1, 'negotiate_ultimately': 1, 'ultimately_reach': 1, 'reach_decisions': 1, 'decisions_line': 1, 'line_objectives': 1, 'objectives_output': 1, 'output_essentially': 1, 'essentially_functioning': 1, 'functioning_system': 1, 'system_showcased': 1, 'showcased_capabilities': 1, 'capabilities_rational': 1, 'environment_highlighting': 1, 'highlighting_potential': 1, 'potential_applications': 1, 'applications_various': 1, 'various_domains': 1, 'domains_smart': 1, 'smart_cities': 1, 'cities_supply': 1, 'supply_chain': 1, 'chain_management': 1, 'management_autonomous': 1, 'autonomous_systems': 1, 'systems_goal': 1, 'goal_provide': 1, 'provide_tangible': 1, 'tangible_example': 1, 'example_rational': 1, 'systems_leveraged': 1, 'leveraged_tackle': 1, 'tackle_complex': 1, 'complex_problems': 1, 'problems_distributed': 1, 'distributed_open': 1, 'source_setting': 1})\n",
      "(925) testing.csv: W, other: A\n",
      "we implemented an automated object recognition system using computer vision technology the project focused on developing a system that can accurately identify and classify objects within images or video footage to accomplish this we utilized various tools and technologies including python programming language opencv library for computer vision tasks such as image processing and object detection and deep learning frameworks like tensorflow or pytorch for training and deploying machine learning models the systems functionality involved preprocessing input images or video frames to extract features then using pre trained deep learning models such as convolutional neural networks cnns to recognize and classify objects within the media we also implemented additional post processing techniques to improve object recognition accuracy such as non maximum suppression for eliminating duplicate object detections the output of the project was a robust and efficient object recognition system capable of processing input media and accurately identifying various objects within the content the system was designed to be scalable and could potentially be used for real time object recognition applications like surveillance systems autonomous vehicles or augmented reality applications overall the projects nature revolved around leveraging computer vision technologies machine learning and deep learning techniques to develop an automated object recognition system with practical applications in diverse industries\n",
      "Counter({'object': 7, 'recognition': 5, 'system': 5, 'learning': 5, 'object_recognition': 5, 'computer': 3, 'vision': 3, 'objects': 3, 'within': 3, 'processing': 3, 'deep': 3, 'applications': 3, 'recognition_system': 3, 'computer_vision': 3, 'objects_within': 3, 'deep_learning': 3, 'implemented': 2, 'automated': 2, 'using': 2, 'accurately': 2, 'classify': 2, 'images': 2, 'video': 2, 'various': 2, 'technologies': 2, 'like': 2, 'machine': 2, 'models': 2, 'systems': 2, 'input': 2, 'media': 2, 'techniques': 2, 'automated_object': 2, 'classify_objects': 2, 'images_video': 2, 'machine_learning': 2, 'learning_models': 2, 'technology': 1, 'focused': 1, 'developing': 1, 'identify': 1, 'footage': 1, 'accomplish': 1, 'utilized': 1, 'tools': 1, 'including': 1, 'python': 1, 'programming': 1, 'language': 1, 'opencv': 1, 'library': 1, 'tasks': 1, 'image': 1, 'detection': 1, 'frameworks': 1, 'tensorflow': 1, 'pytorch': 1, 'training': 1, 'deploying': 1, 'functionality': 1, 'involved': 1, 'preprocessing': 1, 'frames': 1, 'extract': 1, 'features': 1, 'pre': 1, 'trained': 1, 'convolutional': 1, 'neural': 1, 'networks': 1, 'cnns': 1, 'recognize': 1, 'also': 1, 'additional': 1, 'post': 1, 'improve': 1, 'accuracy': 1, 'non': 1, 'maximum': 1, 'suppression': 1, 'eliminating': 1, 'duplicate': 1, 'detections': 1, 'output': 1, 'robust': 1, 'efficient': 1, 'capable': 1, 'identifying': 1, 'content': 1, 'designed': 1, 'scalable': 1, 'potentially': 1, 'used': 1, 'real': 1, 'time': 1, 'surveillance': 1, 'autonomous': 1, 'vehicles': 1, 'augmented': 1, 'reality': 1, 'overall': 1, 'projects': 1, 'nature': 1, 'revolved': 1, 'around': 1, 'leveraging': 1, 'develop': 1, 'practical': 1, 'diverse': 1, 'industries': 1, 'implemented_automated': 1, 'system_using': 1, 'using_computer': 1, 'vision_technology': 1, 'technology_focused': 1, 'focused_developing': 1, 'developing_system': 1, 'system_accurately': 1, 'accurately_identify': 1, 'identify_classify': 1, 'within_images': 1, 'video_footage': 1, 'footage_accomplish': 1, 'accomplish_utilized': 1, 'utilized_various': 1, 'various_tools': 1, 'tools_technologies': 1, 'technologies_including': 1, 'including_python': 1, 'python_programming': 1, 'programming_language': 1, 'language_opencv': 1, 'opencv_library': 1, 'library_computer': 1, 'vision_tasks': 1, 'tasks_image': 1, 'image_processing': 1, 'processing_object': 1, 'object_detection': 1, 'detection_deep': 1, 'learning_frameworks': 1, 'frameworks_like': 1, 'like_tensorflow': 1, 'tensorflow_pytorch': 1, 'pytorch_training': 1, 'training_deploying': 1, 'deploying_machine': 1, 'models_systems': 1, 'systems_functionality': 1, 'functionality_involved': 1, 'involved_preprocessing': 1, 'preprocessing_input': 1, 'input_images': 1, 'video_frames': 1, 'frames_extract': 1, 'extract_features': 1, 'features_using': 1, 'using_pre': 1, 'pre_trained': 1, 'trained_deep': 1, 'models_convolutional': 1, 'convolutional_neural': 1, 'neural_networks': 1, 'networks_cnns': 1, 'cnns_recognize': 1, 'recognize_classify': 1, 'within_media': 1, 'media_also': 1, 'also_implemented': 1, 'implemented_additional': 1, 'additional_post': 1, 'post_processing': 1, 'processing_techniques': 1, 'techniques_improve': 1, 'improve_object': 1, 'recognition_accuracy': 1, 'accuracy_non': 1, 'non_maximum': 1, 'maximum_suppression': 1, 'suppression_eliminating': 1, 'eliminating_duplicate': 1, 'duplicate_object': 1, 'object_detections': 1, 'detections_output': 1, 'output_robust': 1, 'robust_efficient': 1, 'efficient_object': 1, 'system_capable': 1, 'capable_processing': 1, 'processing_input': 1, 'input_media': 1, 'media_accurately': 1, 'accurately_identifying': 1, 'identifying_various': 1, 'various_objects': 1, 'within_content': 1, 'content_system': 1, 'system_designed': 1, 'designed_scalable': 1, 'scalable_potentially': 1, 'potentially_used': 1, 'used_real': 1, 'real_time': 1, 'time_object': 1, 'recognition_applications': 1, 'applications_like': 1, 'like_surveillance': 1, 'surveillance_systems': 1, 'systems_autonomous': 1, 'autonomous_vehicles': 1, 'vehicles_augmented': 1, 'augmented_reality': 1, 'reality_applications': 1, 'applications_overall': 1, 'overall_projects': 1, 'projects_nature': 1, 'nature_revolved': 1, 'revolved_around': 1, 'around_leveraging': 1, 'leveraging_computer': 1, 'vision_technologies': 1, 'technologies_machine': 1, 'learning_deep': 1, 'learning_techniques': 1, 'techniques_develop': 1, 'develop_automated': 1, 'system_practical': 1, 'practical_applications': 1, 'applications_diverse': 1, 'diverse_industries': 1})\n",
      "(933) testing.csv: W, other: S\n",
      "for the capstone project we implemented a web application security enhancement system that focused on vulnerability detection and risk assessment we utilized a combination of tools and technologies to achieve this including burp suite for web application scanning and penetration testing owasp zap for automated security testing and nmap for network discovery and security auditing in addition we used python for scripting and automation and django for developing the web application interface the main output of the project was a web application that allowed users to input a target url and initiate a comprehensive security assessment the system would then conduct automated scans and tests to detect vulnerabilities such as sql injection cross site scripting and insecure configurations the detected vulnerabilities were then categorized and assessed for their potential impact on the applications security furthermore the system provided a risk assessment report that ranked the detected vulnerabilities based on their severity and potential impact this report served as a valuable resource for developers and security professionals to prioritize and remediate the identified security issues effectively overall the project aimed to empower web application developers and security practitioners with a user friendly tool for proactively identifying and addressing potential security risks by leveraging automated scanning testing and risk assessment techniques our system aimed to bolster the security posture of web applications and mitigate the potential for cyber threats and attacks\n",
      "Counter({'security': 10, 'web': 6, 'application': 5, 'web_application': 5, 'system': 4, 'assessment': 4, 'potential': 4, 'risk': 3, 'testing': 3, 'automated': 3, 'vulnerabilities': 3, 'risk_assessment': 3, 'scanning': 2, 'scripting': 2, 'detected': 2, 'impact': 2, 'applications': 2, 'report': 2, 'developers': 2, 'aimed': 2, 'detected_vulnerabilities': 2, 'potential_impact': 2, 'developers_security': 2, 'capstone': 1, 'implemented': 1, 'enhancement': 1, 'focused': 1, 'vulnerability': 1, 'detection': 1, 'utilized': 1, 'combination': 1, 'tools': 1, 'technologies': 1, 'achieve': 1, 'including': 1, 'burp': 1, 'suite': 1, 'penetration': 1, 'owasp': 1, 'zap': 1, 'nmap': 1, 'network': 1, 'discovery': 1, 'auditing': 1, 'addition': 1, 'used': 1, 'python': 1, 'automation': 1, 'django': 1, 'developing': 1, 'interface': 1, 'main': 1, 'output': 1, 'allowed': 1, 'users': 1, 'input': 1, 'target': 1, 'url': 1, 'initiate': 1, 'comprehensive': 1, 'conduct': 1, 'scans': 1, 'tests': 1, 'detect': 1, 'sql': 1, 'injection': 1, 'cross': 1, 'site': 1, 'insecure': 1, 'configurations': 1, 'categorized': 1, 'assessed': 1, 'furthermore': 1, 'provided': 1, 'ranked': 1, 'based': 1, 'severity': 1, 'served': 1, 'valuable': 1, 'resource': 1, 'professionals': 1, 'prioritize': 1, 'remediate': 1, 'identified': 1, 'issues': 1, 'effectively': 1, 'overall': 1, 'empower': 1, 'practitioners': 1, 'user': 1, 'friendly': 1, 'tool': 1, 'proactively': 1, 'identifying': 1, 'addressing': 1, 'risks': 1, 'leveraging': 1, 'techniques': 1, 'bolster': 1, 'posture': 1, 'mitigate': 1, 'cyber': 1, 'threats': 1, 'attacks': 1, 'capstone_implemented': 1, 'implemented_web': 1, 'application_security': 1, 'security_enhancement': 1, 'enhancement_system': 1, 'system_focused': 1, 'focused_vulnerability': 1, 'vulnerability_detection': 1, 'detection_risk': 1, 'assessment_utilized': 1, 'utilized_combination': 1, 'combination_tools': 1, 'tools_technologies': 1, 'technologies_achieve': 1, 'achieve_including': 1, 'including_burp': 1, 'burp_suite': 1, 'suite_web': 1, 'application_scanning': 1, 'scanning_penetration': 1, 'penetration_testing': 1, 'testing_owasp': 1, 'owasp_zap': 1, 'zap_automated': 1, 'automated_security': 1, 'security_testing': 1, 'testing_nmap': 1, 'nmap_network': 1, 'network_discovery': 1, 'discovery_security': 1, 'security_auditing': 1, 'auditing_addition': 1, 'addition_used': 1, 'used_python': 1, 'python_scripting': 1, 'scripting_automation': 1, 'automation_django': 1, 'django_developing': 1, 'developing_web': 1, 'application_interface': 1, 'interface_main': 1, 'main_output': 1, 'output_web': 1, 'application_allowed': 1, 'allowed_users': 1, 'users_input': 1, 'input_target': 1, 'target_url': 1, 'url_initiate': 1, 'initiate_comprehensive': 1, 'comprehensive_security': 1, 'security_assessment': 1, 'assessment_system': 1, 'system_conduct': 1, 'conduct_automated': 1, 'automated_scans': 1, 'scans_tests': 1, 'tests_detect': 1, 'detect_vulnerabilities': 1, 'vulnerabilities_sql': 1, 'sql_injection': 1, 'injection_cross': 1, 'cross_site': 1, 'site_scripting': 1, 'scripting_insecure': 1, 'insecure_configurations': 1, 'configurations_detected': 1, 'vulnerabilities_categorized': 1, 'categorized_assessed': 1, 'assessed_potential': 1, 'impact_applications': 1, 'applications_security': 1, 'security_furthermore': 1, 'furthermore_system': 1, 'system_provided': 1, 'provided_risk': 1, 'assessment_report': 1, 'report_ranked': 1, 'ranked_detected': 1, 'vulnerabilities_based': 1, 'based_severity': 1, 'severity_potential': 1, 'impact_report': 1, 'report_served': 1, 'served_valuable': 1, 'valuable_resource': 1, 'resource_developers': 1, 'security_professionals': 1, 'professionals_prioritize': 1, 'prioritize_remediate': 1, 'remediate_identified': 1, 'identified_security': 1, 'security_issues': 1, 'issues_effectively': 1, 'effectively_overall': 1, 'overall_aimed': 1, 'aimed_empower': 1, 'empower_web': 1, 'application_developers': 1, 'security_practitioners': 1, 'practitioners_user': 1, 'user_friendly': 1, 'friendly_tool': 1, 'tool_proactively': 1, 'proactively_identifying': 1, 'identifying_addressing': 1, 'addressing_potential': 1, 'potential_security': 1, 'security_risks': 1, 'risks_leveraging': 1, 'leveraging_automated': 1, 'automated_scanning': 1, 'scanning_testing': 1, 'testing_risk': 1, 'assessment_techniques': 1, 'techniques_system': 1, 'system_aimed': 1, 'aimed_bolster': 1, 'bolster_security': 1, 'security_posture': 1, 'posture_web': 1, 'web_applications': 1, 'applications_mitigate': 1, 'mitigate_potential': 1, 'potential_cyber': 1, 'cyber_threats': 1, 'threats_attacks': 1})\n",
      "(1050) testing.csv: A, other: G\n",
      "for the project we utilized various tools technologies and libraries to develop an intelligent decision making system our focus was on creating a system that could analyze complex data and provide informed recommendations using machine learning algorithms we used python as the primary programming language due to its extensive libraries for data manipulation and analysis such as numpy pandas and scikit learn to preprocess and clean the data we used pandas and for numerical computations we leveraged the capabilities of numpy our decision making system utilized machine learning models such as decision trees random forests and neural networks implemented using scikit learn and tensorflow these models were trained on historical data to learn patterns and make predictions for future decision making scenarios in addition we employed techniques for feature engineering and model evaluation to ensure the systems effectiveness in providing accurate and reliable recommendations the project also involved the implementation of a user interface using web technologies like html css and javascript allowing users to interact with the decision making system through a user friendly interface the output of this project was an intelligent decision making system capable of processing input data analyzing patterns and providing recommendations based on the learned insights the systems recommendations were supported by the underlying machine learning algorithms which enabled it to adapt and improve its decision making capabilities over time overall the project showcased the integration of machine learning techniques with data preprocessing model training and user interface development to create a robust intelligent decision making system\n",
      "Counter({'decision': 8, 'making': 7, 'decision_making': 7, 'system': 6, 'data': 6, 'making_system': 5, 'recommendations': 4, 'machine': 4, 'learning': 4, 'machine_learning': 4, 'intelligent': 3, 'using': 3, 'learn': 3, 'user': 3, 'interface': 3, 'intelligent_decision': 3, 'utilized': 2, 'technologies': 2, 'libraries': 2, 'algorithms': 2, 'used': 2, 'numpy': 2, 'pandas': 2, 'scikit': 2, 'capabilities': 2, 'models': 2, 'patterns': 2, 'techniques': 2, 'model': 2, 'systems': 2, 'providing': 2, 'learning_algorithms': 2, 'scikit_learn': 2, 'user_interface': 2, 'various': 1, 'tools': 1, 'develop': 1, 'focus': 1, 'creating': 1, 'analyze': 1, 'complex': 1, 'provide': 1, 'informed': 1, 'python': 1, 'primary': 1, 'programming': 1, 'language': 1, 'due': 1, 'extensive': 1, 'manipulation': 1, 'analysis': 1, 'preprocess': 1, 'clean': 1, 'numerical': 1, 'computations': 1, 'leveraged': 1, 'trees': 1, 'random': 1, 'forests': 1, 'neural': 1, 'networks': 1, 'implemented': 1, 'tensorflow': 1, 'trained': 1, 'historical': 1, 'make': 1, 'predictions': 1, 'future': 1, 'scenarios': 1, 'addition': 1, 'employed': 1, 'feature': 1, 'engineering': 1, 'evaluation': 1, 'ensure': 1, 'effectiveness': 1, 'accurate': 1, 'reliable': 1, 'also': 1, 'involved': 1, 'implementation': 1, 'web': 1, 'like': 1, 'html': 1, 'css': 1, 'javascript': 1, 'allowing': 1, 'users': 1, 'interact': 1, 'friendly': 1, 'output': 1, 'capable': 1, 'processing': 1, 'input': 1, 'analyzing': 1, 'based': 1, 'learned': 1, 'insights': 1, 'supported': 1, 'underlying': 1, 'enabled': 1, 'adapt': 1, 'improve': 1, 'time': 1, 'overall': 1, 'showcased': 1, 'integration': 1, 'preprocessing': 1, 'training': 1, 'development': 1, 'create': 1, 'robust': 1, 'utilized_various': 1, 'various_tools': 1, 'tools_technologies': 1, 'technologies_libraries': 1, 'libraries_develop': 1, 'develop_intelligent': 1, 'system_focus': 1, 'focus_creating': 1, 'creating_system': 1, 'system_analyze': 1, 'analyze_complex': 1, 'complex_data': 1, 'data_provide': 1, 'provide_informed': 1, 'informed_recommendations': 1, 'recommendations_using': 1, 'using_machine': 1, 'algorithms_used': 1, 'used_python': 1, 'python_primary': 1, 'primary_programming': 1, 'programming_language': 1, 'language_due': 1, 'due_extensive': 1, 'extensive_libraries': 1, 'libraries_data': 1, 'data_manipulation': 1, 'manipulation_analysis': 1, 'analysis_numpy': 1, 'numpy_pandas': 1, 'pandas_scikit': 1, 'learn_preprocess': 1, 'preprocess_clean': 1, 'clean_data': 1, 'data_used': 1, 'used_pandas': 1, 'pandas_numerical': 1, 'numerical_computations': 1, 'computations_leveraged': 1, 'leveraged_capabilities': 1, 'capabilities_numpy': 1, 'numpy_decision': 1, 'system_utilized': 1, 'utilized_machine': 1, 'learning_models': 1, 'models_decision': 1, 'decision_trees': 1, 'trees_random': 1, 'random_forests': 1, 'forests_neural': 1, 'neural_networks': 1, 'networks_implemented': 1, 'implemented_using': 1, 'using_scikit': 1, 'learn_tensorflow': 1, 'tensorflow_models': 1, 'models_trained': 1, 'trained_historical': 1, 'historical_data': 1, 'data_learn': 1, 'learn_patterns': 1, 'patterns_make': 1, 'make_predictions': 1, 'predictions_future': 1, 'future_decision': 1, 'making_scenarios': 1, 'scenarios_addition': 1, 'addition_employed': 1, 'employed_techniques': 1, 'techniques_feature': 1, 'feature_engineering': 1, 'engineering_model': 1, 'model_evaluation': 1, 'evaluation_ensure': 1, 'ensure_systems': 1, 'systems_effectiveness': 1, 'effectiveness_providing': 1, 'providing_accurate': 1, 'accurate_reliable': 1, 'reliable_recommendations': 1, 'recommendations_also': 1, 'also_involved': 1, 'involved_implementation': 1, 'implementation_user': 1, 'interface_using': 1, 'using_web': 1, 'web_technologies': 1, 'technologies_like': 1, 'like_html': 1, 'html_css': 1, 'css_javascript': 1, 'javascript_allowing': 1, 'allowing_users': 1, 'users_interact': 1, 'interact_decision': 1, 'system_user': 1, 'user_friendly': 1, 'friendly_interface': 1, 'interface_output': 1, 'output_intelligent': 1, 'system_capable': 1, 'capable_processing': 1, 'processing_input': 1, 'input_data': 1, 'data_analyzing': 1, 'analyzing_patterns': 1, 'patterns_providing': 1, 'providing_recommendations': 1, 'recommendations_based': 1, 'based_learned': 1, 'learned_insights': 1, 'insights_systems': 1, 'systems_recommendations': 1, 'recommendations_supported': 1, 'supported_underlying': 1, 'underlying_machine': 1, 'algorithms_enabled': 1, 'enabled_adapt': 1, 'adapt_improve': 1, 'improve_decision': 1, 'making_capabilities': 1, 'capabilities_time': 1, 'time_overall': 1, 'overall_showcased': 1, 'showcased_integration': 1, 'integration_machine': 1, 'learning_techniques': 1, 'techniques_data': 1, 'data_preprocessing': 1, 'preprocessing_model': 1, 'model_training': 1, 'training_user': 1, 'interface_development': 1, 'development_create': 1, 'create_robust': 1, 'robust_intelligent': 1})\n"
     ]
    }
   ],
   "source": [
    "#Compare Results Function\n",
    "\n",
    "def Compare(file): \n",
    "    data = pd.read_csv(\"train.csv\")\n",
    "    descs = data[\"Description\"].values\n",
    "    comparer = pd.read_csv(f\"{file}\")\n",
    "    X1 = comparer[\"Class\"].values\n",
    "    comparee = pd.read_csv(\"prediction25.csv\")\n",
    "    X2 = comparee[\"Class\"].values\n",
    "\n",
    "    for i, val in enumerate(X1):\n",
    "        if val != X2[i]:\n",
    "            print(f\"({i+1}) {file}: {val}, other: {X2[i]}\")\n",
    "            print(descs[i])\n",
    "            words = improved_preprocess(descs[i])\n",
    "            print(Counter(words))\n",
    "\n",
    "name = \"testing.csv\"\n",
    "Submission(name)\n",
    "Compare(name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
