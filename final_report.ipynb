{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc3190bf",
   "metadata": {},
   "source": [
    "# CS361 Group Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e682a2e1",
   "metadata": {},
   "source": [
    "Group Members: Lucy Harris (lhar917), Hayley Sharpe (hsha609), Kunal Bhaskar (kbha962), Jackson Fontaine (jfon971), Seth Gousmett (sgou398)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06600171",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df140ee0",
   "metadata": {},
   "source": [
    "(a) Load the dataset and construct a feature vector for each article in the entire dataset. You\n",
    "need to report the number of articles, and the number of extracted features. Show 5 example\n",
    "articles with their extracted features using a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c674bef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9af7439f",
   "metadata": {},
   "source": [
    "(b) Conduct term frequency analysis and report three plots: (i) top-50 term frequency distribution\n",
    "across the entire dataset, (ii) term frequency distribution for respective class of articles, and (iii)\n",
    "class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de78e4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9ced531",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa08076",
   "metadata": {},
   "source": [
    "#### Part A - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a14568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "077e105b",
   "metadata": {},
   "source": [
    "#### Part B - K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686bb455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03a44f35",
   "metadata": {},
   "source": [
    "#### Part C - Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a436aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eace74fe",
   "metadata": {},
   "source": [
    "#### Part D - Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e9f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    data = make_df(\"../../train.csv\")\n",
    "    y = data[\"Category\"]\n",
    "    test_data = make_df(\"../../test.csv\")\n",
    "    vector, vector_test = vectorize(data, test_data)\n",
    "\n",
    "    cels = []\n",
    "    num_nods = [5, 20, 40]\n",
    "    for num in num_nods:\n",
    "        probs, clf = make_classifier(vector, y, vector_test, num)\n",
    "        cels.append(np.mean(clf.loss_curve_))\n",
    "\n",
    "    plt.plot(num_nods, cels, marker = \"*\", color = \"#F543BA\", markersize = 10)\n",
    "    plt.xlabel(\"number of nodes per layer\")\n",
    "    plt.ylabel(\"Cross entropy loss\")\n",
    "    plt.title(\"Cross entropy loss by number of nodes per layer\")\n",
    "    plt.show()\n",
    "    \n",
    "def vectorize(data, test_data):\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(data[\"Text\"])\n",
    "    vector = vectorizer.transform(data[\"Text\"])\n",
    "    test_vector = vectorizer.transform(test_data[\"Text\"])\n",
    "    return vector, test_vector\n",
    "\n",
    "def make_df(filename):\n",
    "    return pd.read_csv(filename)\n",
    "\n",
    "def make_classifier(vector, y, vector_test, num_nodes = 5):\n",
    "    # creating the untrained model\n",
    "    clf = MLPClassifier(activation = 'relu',\n",
    "                        solver = 'sgd',\n",
    "                        learning_rate_init = 0.01,\n",
    "                        max_iter = 100,\n",
    "                        hidden_layer_sizes = (num_nodes,))\n",
    "\n",
    "    # initialise weights\n",
    "    clf.partial_fit(vector, y, classes=np.unique(y))\n",
    "\n",
    "    # manually set weights to [0, 0.1]\n",
    "    for i in range(len(clf.coefs_)):\n",
    "        clf.coefs_[i] = np.random.uniform(0, 0.1, size=clf.coefs_[i].shape)\n",
    "    for i in range(len(clf.intercepts_)): \n",
    "        clf.intercepts_[i] = np.random.uniform(0, 0.1, size=clf.intercepts_[i].shape)\n",
    "\n",
    "    # training\n",
    "    clf.fit(vector, y)\n",
    "\n",
    "    probs = clf.predict_proba(vector_test)\n",
    "\n",
    "    return probs, clf\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabf1630",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2b8cf0",
   "metadata": {},
   "source": [
    "#### Part A - Training/Validation Proportion Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5ae0a3",
   "metadata": {},
   "source": [
    "Neural Networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61076dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    data = make_df(\"../../train.csv\")\n",
    "    y = data[\"Category\"]\n",
    "    test_data = make_df(\"../../test.csv\")\n",
    "    y_test = test_data[\"Category\"]       \n",
    "    vector, vector_test = vectorize(data, test_data)\n",
    "\n",
    "    test_f1s = []\n",
    "    train_f1s = []\n",
    "    ms = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    for m in ms:\n",
    "        train_f1, test_f1 = train_on_m(y, vector, vector_test, m, y_test)\n",
    "        test_f1s.append(test_f1)\n",
    "        train_f1s.append(train_f1)\n",
    "\n",
    "    plt.plot(ms, test_f1s, marker = \"o\", color = \"#BA52FF\", markersize = 7)\n",
    "    plt.title(\"Test F1 Scores\")\n",
    "    plt.xlabel(\"m Values\")\n",
    "    plt.ylabel(\"F1 score\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(ms, train_f1s, marker = \"x\", color = \"#F431CB\", markersize = 8)\n",
    "    plt.title(\"Train F1 Scores\")\n",
    "    plt.xlabel(\"m Values\")\n",
    "    plt.ylabel(\"F1 score\")\n",
    "    plt.show()\n",
    "    \n",
    "def train_on_m(train_y, vector, vector_test, m, y_test):\n",
    "    sub_vector = vector[0:round(vector.shape[0]*m)]\n",
    "    \n",
    "    probs, clf = make_classifier(sub_vector, train_y[0:round(len(train_y)*m)], vector_test)\n",
    "\n",
    "    preds_test = clf.predict(vector_test)\n",
    "    preds_train = clf.predict(sub_vector)\n",
    "\n",
    "    train_f1 = calc_f1(preds_train, train_y)\n",
    "    test_f1 = calc_f1(preds_test, y_test)\n",
    "\n",
    "    return train_f1, test_f1\n",
    "\n",
    "def calc_f1(preds, actual):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "\n",
    "# Consider tech to be the positive class\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] == \"tech\" and actual[i] == \"tech\":\n",
    "            tp += 1\n",
    "        elif preds[i] == \"tech\" and actual[i] == \"entertainment\":\n",
    "            fp += 1\n",
    "        elif preds[i] == \"entertainment\" and actual[i] == \"entertainment\":\n",
    "            tn += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2*((precision*recall) / (precision+recall))\n",
    "\n",
    "    return f1\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5e6259",
   "metadata": {},
   "source": [
    "#### Part B - 5-Fold Cross-Validation Model Performance Assessments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656efb39",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f36d0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def vectorize(data, test_data):\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(data[\"Text\"])\n",
    "    vector = vectorizer.transform(data[\"Text\"])\n",
    "    test_vector = vectorizer.transform(test_data[\"Text\"])\n",
    "\n",
    "    return vector, test_vector\n",
    "\n",
    "def make_df(filename):\n",
    "    return pd.read_csv(filename)\n",
    "\n",
    "def main():\n",
    "    data = make_df(\"../../train.csv\")\n",
    "    y = data[\"Category\"]\n",
    "    test_data = make_df(\"../../test.csv\")\n",
    "    y_test = test_data[\"Category\"]       \n",
    "    vector, vector_test = vectorize(data, test_data)\n",
    "    probs, model = make_classifier(vector, y, vector_test)\n",
    "\n",
    "    cross_validate(model, vector, y, vector_test, y_test)\n",
    "\n",
    "def make_classifier(vector, y, vector_test, num_nodes = 5):\n",
    "    # creating the untrained model\n",
    "    clf = MLPClassifier(activation = 'relu',\n",
    "                        solver = 'sgd',\n",
    "                        learning_rate_init = 0.01,\n",
    "                        max_iter = 100,\n",
    "                        hidden_layer_sizes = (num_nodes,))\n",
    "\n",
    "    # initialise weights\n",
    "    clf.partial_fit(vector, y, classes=np.unique(y))\n",
    "\n",
    "    # manually set weights to [0, 0.1]\n",
    "    for i in range(len(clf.coefs_)):\n",
    "        clf.coefs_[i] = np.random.uniform(0, 0.1, size=clf.coefs_[i].shape)\n",
    "    for i in range(len(clf.intercepts_)): \n",
    "        clf.intercepts_[i] = np.random.uniform(0, 0.1, size=clf.intercepts_[i].shape)\n",
    "\n",
    "    # training\n",
    "    clf.fit(vector, y)\n",
    "\n",
    "    probs = clf.predict_proba(vector_test)\n",
    "    return probs, clf\n",
    "\n",
    "def cross_validate(clf, train_descriptions, train_y, test_descriptions, test_y):\n",
    "    scores = cross_val_score(clf, train_descriptions, train_y, cv=5, scoring=\"f1_macro\")\n",
    "    print(\"Scores for normal classifier:\", scores, \"with average\", str(sum(scores) / len(scores)))\n",
    "\n",
    "    # calculating learning rate accuracy scores\n",
    "    learning_rates = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 0.75, 1, 2, 5]\n",
    "    for learning_rate in learning_rates:\n",
    "        # retrain clf\n",
    "        clf = MLPClassifier(activation = 'relu', solver = 'sgd', learning_rate_init = learning_rate,\n",
    "                        max_iter = 100, hidden_layer_sizes = (5,)).fit(train_descriptions, train_y)\n",
    "        get_scores(clf, test_descriptions, test_y, \"learning rate\", learning_rate)\n",
    "\n",
    "    # calculating epoch count accuracy scores\n",
    "    max_iter_values = [5, 50, 100, 200, 500, 1000, 5000]\n",
    "    for max_iter_val in max_iter_values:\n",
    "        # retrain clf\n",
    "        clf = MLPClassifier(activation = 'relu', solver = 'sgd', learning_rate_init = 0.01,\n",
    "                        max_iter = max_iter_val, hidden_layer_sizes = (5,)).fit(train_descriptions, train_y)\n",
    "        get_scores(clf, test_descriptions, test_y, \"max iterations\", max_iter_val)\n",
    "\n",
    "    # calculating activation function accuracy scores\n",
    "    activation_functions = [\"relu\", \"identity\", \"logistic\", \"tanh\"]\n",
    "    for function in activation_functions:\n",
    "        # retrain clf\n",
    "        clf = MLPClassifier(activation = function, solver = 'sgd', learning_rate_init = 0.01,\n",
    "                        max_iter = 100, hidden_layer_sizes = (5,)).fit(train_descriptions, train_y)\n",
    "        get_scores(clf, test_descriptions, test_y, \"activation function\", function)\n",
    "\n",
    "    # calculating hidden layer count accuracy scores\n",
    "    num_layers = [1, 2, 3, 4, 5, 10, 20, 40]\n",
    "    for num in num_layers:\n",
    "        # retrain clf\n",
    "        sizes = (5,) * num\n",
    "        clf = MLPClassifier(activation = 'relu', solver = 'sgd', learning_rate_init = 0.01,\n",
    "                        max_iter = 100, hidden_layer_sizes = sizes).fit(train_descriptions, train_y)\n",
    "        get_scores(clf, test_descriptions, test_y, \"number of layers = \", num)\n",
    "\n",
    "    # taking subset of the best epochs and learning rates, and finding best accuracy for this\n",
    "    max_iter_values = [100, 200, 500, 1000, 5000]\n",
    "    learning_rates = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "    for iter_val in max_iter_values:\n",
    "        for learning_rate in learning_rates:\n",
    "        # retrain clf for each combo of learning rate and max iter\n",
    "            clf = MLPClassifier(activation = 'relu', solver = 'sgd', learning_rate_init = learning_rate,\n",
    "                            max_iter = iter_val, hidden_layer_sizes = 2).fit(train_descriptions, train_y)\n",
    "            get_scores(clf, test_descriptions, test_y, f\"Max iterations = {iter_val},\", f\"learning rate = {learning_rate}\") # change this text\n",
    "\n",
    "\n",
    "def get_scores(clf, test_descriptions, test_y, hyperparam, value):\n",
    "        scores = cross_val_score(clf, test_descriptions, test_y, cv=5, scoring=\"f1_macro\")\n",
    "        avg = sum(scores) / len(scores)\n",
    "        # UNCOMMENT THIS PRINT TO SEE CV SCORES AND AVERAGES\n",
    "        # print(f\"Score for {hyperparam} {value}:\", scores, \"with average\", str(avg))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012c65b3",
   "metadata": {},
   "source": [
    "#### Part C - Report and Compare Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4f63f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
